Lundi 12 décembre 2016, le séminaire « Re / lire les sciences sociales » recevait le démographe Hervé Le Bras pour discuter de son dernier ouvrage, Anatomie sociale de la France. Ce que les Big data disent de nous, Paris, Robert Laffont, 2016. La séance était co-organisée par le physicien Pablo Jensen, également second intervenant. Voir la vidéo de la séance  L’intervention de l’invité : Hervé le Bras Le propos général d’Hervé Le Bras dans son intervention était de présenter et d’illustrer, au travers de quelques exemples biens choisis, la méthode qu’il a déployée dans Anatomie sociale de la France.  Il s’agissait dans un premier temps de montrer la nécessité de croiser les données statistiques, au lieu de les utiliser individuellement, afin de mieux comprendre les comportements sociaux. Ainsi, à regarder les chiffres de loin, une différence sensible entre les hommes et les femmes semble s’exprimer dans le vote en faveur de l’extrême droite : aux dernières élections françaises, 30% des hommes et 26% des femmes ont voté pour le Front National. S’agit-il d’une différence de sexe ? A priori seulement, car il faut regarder la composition de l’électorat : parmi les personnes âgées, les femmes sont plus nombreuses que les hommes. Or, les personnes âgées votent moins pour le FN. En rectifiant avec une structure d’âge égale pour les deux sexes, on obtient 28,3% et 27,7%. L’effet du sexe et de l’âge se composent donc : en ne prenant en compte qu’un seul d’entre eux, on risque de masquer la véritable structure de la société. Un autre exemple convoqué par H. Le Bras permet de comprendre le jeu complexe de composition des données : le rapport entre catégorie sociale et vote. Il est en effet tentant de penser que la Profession et Catégorie Sociale (PCS) détermine le vote. Hervé Le Bras présente alors un de ces outils de modélisation. Il simule un vote avec pour seule variable explicative la PCS, et attribue à chaque PCS la probabilité de voter pour tel ou tel candidat. Il apparaît que les résultats de la simulation sont très éloignés des résultats empiriques, et qu’il n’est pas possible de trouver les bons coefficients de probabilité. Ce passage par la formalisation permet ainsi de montrer qu’il est faux de considérer que le vote est déterminé (uniquement ou même principalement) par les PCS. Après cette première présentation, Hervé Le Bras s’est arrêté un moment sur quelques outils de statistique formelle qu’il utilise dans son livre. Dans un souci de synthèse, nous ne présentons pas ces outils dans le détail ici. Contentons-nous de noter l’idée générale de l’auteur : les méthodes statistiques traditionnelles présupposent des relations de composition trop simples entre les effets des variables. M. Le Bras a notamment souligné le fait que les régressions logistiques supposaient souvent une cumulativité des effets, alors que dans certains cas, selon lui, les effets ne se composent pas par l’addition, mais par la multiplication (il a notamment pris l’exemple du chômage). Finalement, Hervé Le Bras conclut son intervention par un retour sur ce que permet l’exploration des « Big Data », sous-titre de son ouvrage. Il reconnait qu’il n’utilise pas vraiment de big data, puisque les « big data » sont de l’ordre du milliard de données tandis que celles de l’INSEE sont de l’ordre du million. Mais qu’importe : ce grand nombre de données permet de gagner de la prévision. Sa démarche, nous dit-il, est rendu possible par l’accès à de nombreuses données de l’INSEE : le croisement des données permet de donner une description fine de la société, grâce aussi à la taille considérable des échantillons. Il prend l’exemple du taux de chômage, expliqué à la fois par l’âge, le niveau d’étude, la présence ou l’absence d’un événement migratoire dans le parcours de vie : les résultats sont dès lors précis et contrastés. Cependant, il ne faut croire que seul le nombre des données peut suffire pour arriver à tout expliquer. La décroissance de l’erreur de prévision est très lente (une seule donnée est associée à 100% d’erreur, mais 100 000 données ne réduit ce pourcentage que de 10 points). Ainsi, H. Le Bras conclut sur une note optimiste : nos craintes d’un Big Brother, qui à l’aide de nombreuses données, anticiperait nos comportements semblent infondées ; les comportements individuels ne sont pas prévisibles, seuls le sont les comportements agrégés. L’intervention du discutant : Pablo Jensen Pablo Jensen a ensuite pris la parole et articulé un propos rapide autour des sciences sociales et de la question de leur formalisation. En effet, aujourd’hui, les big data semblent renouveler les sciences sociales : quelques informaticiens, physiciens (pas tous !) sont attirés par ces données et s’intéressent à la « modélisation du social », avec peut-être pour projet d’une science de la société (enfin !) positive. Pablo Jensen, lui-même physicien, se détache d’une telle posture. Les sciences sociales sont en effet perturbantes si on les approche de la même manière que les objets traditionnels de la physique: les mêmes causes ne provoquent pas forcément les mêmes effets (le taux de chômage diminue avec le diplôme pour les jeunes… mais ce même taux augmente avec le diplôme pour les plus âgés), et l’expérimentation est en sciences sociales plus complexe. Pour conclure sur les big data, Pablo Jensen se place du point de vue de Google, plutôt que celui de l’État : cette avalanche de données sert moins au bien commun qu’à celui de l’entreprise. Ainsi, Google, grâce aux données informatiques (historiques des recherches sur le site, « cookies », et autres « traces » que nous laissons sur le web), peut essayer de deviner si un individu est au chômage, et ainsi placer une publicité adaptée à nos « préférences révélées » (pour un travail ou des vacances). Discussion avec la salle La question de l’imprécision des catégories en général, et des PCS en particulier, a été plusieurs fois soulevée. H. Le Bras est d’accord, mais souligne que plus les catégories sont fines, moins on gagne d’information… Quant à P. Jensen, il explique la nécessité des catégories génériques : il n’existe pas de catégories pertinentes en soi. Il peut néanmoins être utile d’utiliser des catégories plus fines sur un cas précis. Lorsque l'un des participants a demandé à H. Le Bras d’expliquer les raisons d’un fait (le vote FN par les personnes âgées) mis en avant dans le livre, le démographe en a profité pour rappeler que son livre n’est pas fait pour expliquer sociologiquement. Le but de son livre est avant de l’ordre du constat : il s’agit de « mesurer », de faire une « anatomie sociale » de la France.  Ce thème de la démarche de H. Le Bras a permis d’aborder la question du public auquel il s’adresse, puisque son livre n’est ni tout à fait de la vulgarisation, et ni vraiment à l’usage d’universitaire. Le démographe avoue alors regretter le manque de lisibilité de son ouvrage (toutefois toute relative, et malgré ses efforts, notamment sur la mise en page des graphiques) : il est difficile de maîtriser 22 millions de données… La présence d’un démographe et d’un physicien à la même table, et même, une certaine harmonie, a pu surprendre. En témoigne un assez long moment d’échange avec la salle sur la possibilité d’un dialogue entre des disciplines a priori éloignées, ainsi que sur la question de l’interdisciplinarité. P. Jensen nous met en garde : sa propre ouverture disciplinaire et celle de H. Le Bras lui semblent exceptionnelles. Le dialogue ne se passe pas, en général, et il faut le forcer un peu. A chaque fois que le dialogue a lieu, il s’appuie d’ailleurs des opportunités, sur des cas spécifiques. H. Le Bras se méfie lui aussi de la fameuse « interdisciplinarité » : selon lui, c’est à l’intérieur de l’individu qu’elle doit d’abord avoir lieu. Il prend son propre parcours, qui, après une formation à Polytechnique, lui a permis d’approfondir la démographie et de s’intéresser également aux mathématiques, à la sociologie et même à l’architecture. Les deux intervenants sont d’accord pour souligner le principal risque de l’interdisciplinarité : le survol artificiel de la discipline qu’on ne connaît pas. Il s’agit en effet de s’ancrer dans une tradition, de maîtriser des outils qui ont une longue histoire. Enfin, les big data ont suscité quelques interrogations de la part du public, qui a souhaité approfondir ce thème. Que nous permettent-elles de comprendre ? Le Bras et P. Jensen sont d’accord pour y voir parfois un effet de mode : les conférences sur les big data sont souvent creuses, et les enquêtes les utilisant peuvent reposer sur des effets de manche. Pourtant, l’utilisation des big data a bien des effets, mais davantage pour des entreprises privées sur Internet, comme Google, Amazon, qui s’en servent pour proposer des produits adaptés aux goûts, besoins, etc. de chaque utilisateur. Notre rapport aux big data est lui-aussi intéressant. Elles suscitent des réactions très différentes : on peut craindre le pouvoir qu’elles donnent à Google, on peut redouter l’émergence d’un Big Brother, on peut considérer qu’elles vont révolutionner le monde… Toutefois, P. Jensen relativise l’importance de ces big data : spécialiste des systèmes complexes, il explique que les « données complexes » ainsi que l’accumulation immense des données, ne donnent pas forcément de meilleurs résultats. C’est un point qui avait déjà été abordé par H. Le Bras : la capacité à « déterminer » est décroissante avec le nombre de données. Même si on double les données accessibles, on n’augmentera pas tellement la précision de notre connaissance. Le big data, ce n’est donc pas la panacée. En revanche, il considère la « révolution numérique »  comme quelque chose de plus large, à la fois économique, social, politique, et qui peut, elle, davantage révolutionner l’univers de la connaissance. Robin Lenoir et Tom Cluzeau (élèves à l’ENS de Lyon)