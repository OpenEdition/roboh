J’ai découvert la lexicométrie en lisant le livre de Claire Lemercier et Claire Zalc sur les méthodes quantitatives pour les historiens[1]. Mon désir d’explorer ces méthodes a cependant été soumis à rude épreuve puisque je n’arrivais pas à trouver un document de synthèse qui m’aurait pu constituer un point de départ de mes recherches, en me donnant un aperçu de l’histoire de la lexicométrie, des logiciels que l’on pouvait utiliser, et une bibliographie de base. Je n’ai pu finalement réaliser mon projet que grâce à l’aide successive de Claire Lemercier[2] et surtout celle de Benjamin Deruelle, qui m’ont fait comprendre en quoi consistait la lexicométrie et comment elle pouvait être utilisée dans le cadre de mon sujet de recherche[3]. Le souvenir de ces débuts difficiles m’a incitée à écrire une série de billets sur la lexicométrie. Ils proposent une synthèse de mes propres lectures mais aussi des cours que j’ai pu suivre à la Sorbonne et à l’E.N.S. J’espère qu’ils pourront contribuer à faire connaître ces méthodes encore assez confidentielles, et aider ceux et celles qui auraient le désir de les exploiter, en exposant, notamment, mes propres pratiques lexicométriques.   Qu’est-ce que la lexicométrie ? La plupart du temps, les historiens travaillent de la façon suivante (je simplifie ici, bien entendu, considérablement le propos) : ils recopient ou photographient leurs sources puis les lisent et relisent de façon à se les approprier, à en faire la critique et à en proposer une interprétation. Ces lectures permettent de construire un discours historique. La lexicométrie a pris naissance dans la remise en question de cette méthode traditionnelle, et la volonté de la dépasser. Pour ses promoteurs, cette lecture documentaire informative, qui ne s’interroge pas sur la forme même du texte et son sens intrinsèque[4] ne suffit pas. Elle propose une échelle d’analyse complémentaire, celle des formes de mots. Il s’agit de comprendre, grâce à des méthodes mathématiques, la répartition de ces dernières et leur fonctionnement dans un corpus. (Celui-ci étant défini comme la somme de tous les textes sélectionnés par l’historien selon des critères précis et justifiés). Les emplois des mots dans les textes sont quantifiés et comparés, ce qui permet de mieux comprendre la structure du corpus, et d’accéder à une échelle de comparaison plus fine. On ne peut saisir ni l’objet ni l’originalité de cette méthode, si l’on néglige l’histoire de la lexicométrie et les controverses qui ont rythmé son émergence et sa légitimation en tant que méthode. C’est la raison pour laquelle j’ai décidé, pour ce premier billet, de m’attarder quelque peu sur le contexte dans lequel la lexicométrie est née et de montrer ensuite en quoi elle a été disqualifiée au profit des méthodes dites qualitatives. J’expliquerai enfin  la façon avec laquelle une lexicométrie rénovée a acquis de nouveau le statut d’une méthode crédible d’analyse de sources historiques[5]. Les premiers usages : de l’espoir à la désillusion  Une linguistique nouvelle manière… La lexicométrie a été développée dans les années 60 par des linguistes influencés par deux courants intellectuels majeurs : le structuralisme et le quantitativisme. Le structuralisme[6] remet en question la philosophie individualiste[7] qui avait prédominé jusqu’alors. Lévi-Strauss[8] montre en effet que l’individu ne peut pas se définir comme un homoncule[9] qui s’opposerait à la société grâce à sa personnalité et à ses caractéristiques personnelles, mais bien comme un membre dépendant d’une société qui le façonne et lui impose des règles dont il n’a plus conscience. Le sujet s’insère donc dans cette société en se soumettant à ses règles, mais il peut aussi la façonner à son tour par son action sociale[10]. La langue est l’un des lieux où la société s’impose à l’homme, comme le montre Saussure[11]. Il tranche en effet le débat du Cratyle[12] en expliquant que la langue est un choix arbitraire de la société et qu’il n’y a pas de sons « naturels » pour désigner un signifiant. La grammaire, la syntaxe sont autant de lieux où la société s’impose comme instance décisionnaire face à l’usage que l’individu fait des mots. De ce fait, aucune langue ne peut s’expliquer uniquement par l’individu, puisqu’elle se définit à la fois par ce qui appartient au locuteur et par la société qui le façonne. C’est ce qui fera dire à Lacan[13] que la langue est une action par laquelle un individu se reflète dans un autre. Ce passage de la philologie[14], que Oswald Ducrot définit comme le fait de penser le langage comme une « expression de la pensée »[15], au structuralisme qui pense l’individu comme une entité insérée dans une structure qui le dépasse, est déterminant pour la linguistique. Le discours, pensé jusqu’alors comme quelque chose qui avait un sens transparent[16], devient alors porteur d’un sens dont l’individu n’aurait pas conscience lui-même et qu’il faudrait mettre en évidence pour avoir accès aux structures internes de la société.[17] La façon dont l’individu s’exprime, pensée jusqu’alors comme un détail que l’on pouvait délaisser au profit de la recherche des informations contenues dans le texte, devient alors centrale dans l’appréhension des structures internes à la société. Ce type d’analyse est pensé comme le moyen de « déceler l’indécelé dans le texte même »[18]. Dans cette optique, la lecture des sources que l’on avait appliquée jusqu’alors devient insuffisante, puisqu’elle ne permet pas de mettre en évidence la structure d’un corpus, l’œil humain étant très vite dépassé par la grandeur du corpus. Comparer les textes entre eux pour en trouver la structure s’impose comme une exigence fondamentale. Pour ce faire, il faut inventer de nouvelles méthodes. Cette première exigence de compréhension des rapports entre les formes de mots se double d’une seconde, celle  de la quantification. On est en effet au moment où l’histoire quantitative atteint son acmé[19]. Elle considère « que le chiffre peut dès lors révéler le poids des structures »[20]. Si la structure de la société peut être étudiée par la comparaison des évolutions des prix ou de la démographie, la structure des corpus devrait être étudiable grâce aux statistiques textuelles et aux index des noms. À partir de là, la statistique textuelle est clairement pensée comme une sous branche de l’histoire quantitative[21]. Les statistiques sont  le garant du fait que les idées mises en évidence par l’analyse sont effectivement exprimées par le discours de la majorité de la population étudiée. On invente alors les premiers outils statistiques d’analyse de texte : l’analyse distributionnelle américaine[22] et les index de mots, assortis de leurs occurrences. La mue de la philologie en linguistique structuraliste est essentielle puisqu’elle permet de penser le langage comme une porte ouverte sur les structures internes de la société, ce que laissait présager aussi le Lacanisme. La volonté de comprendre ces dernières poussent les linguistes à analyser d’une nouvelle façon les discours en utilisant des méthodes auparavant peu usitées : celles des méthodes quantitatives.  … qui fait envie aux historiens… Dans les années 1960, les historiens sont très intéressés par les premiers succès de cette linguistique nouvelle manière. Ils la voient   une comme une méthode d’analyse qui pourra assurer la scientificité de leurs résultats et conférer à l’histoire sa qualité de science[23]. Ce raisonnement est très bien exprimé par Régine Robin : « Si les linguistes fournissent à l’historien des techniques, grâce à ce protocole méthodologique, il va pouvoir lire de façon neuve et interpréter les textes. (…) On pense pouvoir pallier ces manques, ces absences, par le recours à des techniques dûment éprouvées, dont le caractère formalisé offre toutes les garanties. »[24] Ils pensent alors que le rôle du linguiste est d’ « apprendre à lire ce qu’il y a dans le texte » à l’historien et « de l’aider à mettre le texte à plat et de l’ordonner. »[25]  Ce dernier, fort de cette méthode est alors capable d’en tirer des conclusions sur les rapports sociaux. La conviction partagée par la majorité des historiens d’avoir trouvé une méthode scientifique certaine explique l’effet de mode que connaît la lexicométrie dans les années 1970. Ceci est visible chez Antoine Prost puisqu’il se sent obligé d’expliquer que sa démarche n’est pas conditionnée par cette « mode[26] ». Cet engouement fut néanmoins beaucoup plus le fait de linguistes pour l’histoire que d’historiens pour la linguistique. Dans les années 1970, deux institutions décident de développer la lexicométrie et de s’en servir comme d’un outil de compréhension des textes. Il s’agit de l’E.N.S de Saint-Cloud et de l’université de Nanterre (Paris X). Elles ont pour particularité de faire collaborer des linguistes (à l’instar d’André Salem) avec des historiens. La focale utilisée est assez étroite. Ils s’intéressent principalement aux discours des hommes politiques des XIXe et XXe siècles. La littérature comme la période moderne sont très peu traitées, à l’exception de la Révolution française qui donne lieu à des recherches approfondies sur les discours des contemporains[27]. Le développement d’une linguistique nouvelle a donné l’espoir aux historiens de refonder leur discipline en tant que véritable science. Deux équipes de recherche se sont principalement attelées à cette tâche. Malheureusement, ces espoirs ont été assez vite battus en brèche par les critiques conjointes des linguistes et des historiens.  … qui déchantent assez vite L’usage de ces nouvelles méthodes donne très vite lieu à une polémique violente entre les partisans et les détracteurs de la lexicométrie. Les critiques sont principalement le fait des linguistes et des historiens. Les premiers reprochent à leurs collègues d’avoir accepté que le mot soit l’échelle d’analyse de la lexicométrie, puisque la définition même du mot et de sa légitimité à être considéré comme l’élément de base de la phrase est un débat non réglé de leur propre discipline[28]. Ils considèrent aussi que les index de mots laissent la phrase, la syntaxe et la grammaire exsangues, ce qui rend impossible une bonne compréhension des discours eux-mêmes. Les seconds reprochent quant à eux à leurs collègues de plaquer des considérations qui leurs sont contemporaines (notamment le concept de lutte des classes) sur les siècles passés. Cet anachronisme se retrouverait selon eux dans l’utilisation, par les historiens,  des  « mots pivots. »[29]. Cette méthode consiste à chercher des mots spécifiques dans un discours en les ayant présélectionnés à l’avance. Cette intrusion de la subjectivité de l’historien et les erreurs sur lesquelles elle débouche rendrait impossible l’utilisation de la lexicométrie. Ils vilipendent enfin l’usage des statistiques en expliquant que ces chiffres n’ont aucun sens en eux-mêmes[30] et qu’ils servent simplement à donner l’illusion au lecteur que le propos est scientifique[31]. On peut voir que si ces critiques sont, pour certaines, propres à la lexicométrie, d’autres prennent aussi place dans un discours plus global de dénonciation de l’emballement de l’histoire quantitative[32] qui aurait effacé la singularité des structures, mis en place un culte fétichiste pour le chiffre, et n’aurait rien gagné à tenter de rendre l’histoire plus scientifique par l’usage des statistiques. La lexicométrie n’a pas été capable de faire face à des arguments aussi violents et parfois assez justifiés. Elle est tombée en déshérence au moment même où les ordinateurs auraient pu leur conférer les capacités de calcul dont les spécialistes de la lexicométrie rêvaient. Le renouveau de la lexicométrie L’opprobre jeté sur la lexicométrie n’a pas rebuté tous les historiens. Certains comme André Salem, Alain Guerreau ou Jean-Philippe Genet ont continué à s’y intéresser, et se sont servi des critiques qui leur avaient été adressées pour améliorer les logiciels de lexicométrie. C’est grâce à cette ténacité que les trois logiciels les plus importants de la lexicométrie : Alceste[33], Lexico3 et Hyperbase[34] ont pu voir le jour. Ces logiciels, s’ils ne pouvaient résoudre le problème du choix du mot comme unité de base d’un texte, rendent caduques la plupart des critiques adressées aux usages de la lexicométrie. Chacun d’eux utilise la contextualisation, à savoir  l’étude de l’environnement des mots du corpus. Dire que la lexicométrie, en isolant les mots rend incompréhensible leur véritable sens est donc obsolète. Cette étude du sens profond des mots, permise par la contextualisation, se double d’une recherche sur la phrase elle-même. Ni la syntaxe, ni la grammaire ne sont sacrifiées au profit du mot. L’étude de Damon Mayaffre[35] sur les pronoms et les temps utilisés dans les discours des présidents de la cinquième république suffit à le démontrer. Enfin, le problème de l’intrusion de l’historien dans le choix des mots a été réglé par la mobilisation d’analyses factorielles des correspondances[36] (AFC), les études des spécificités du corpus et de ses co-occurrences. Cette mutation profonde de la lexicométrie, permise par un énorme travail sur les logiciels, et une profonde réflexion épistémologique, a permis à de nombreuses études de voir le jour. La nature des sources a profondément changé. Même si ces travaux interrogent souvent l’aspect politique des textes, on est passé d’une étude quasi exclusive des discours politiques à une étude de « nouvelles sources » telles que les prologues de chansons de gestes[37] et la presse[38]. Ces nouveaux travaux sont particulièrement intéressants et innovants. Pourtant, ils restent très peu reconnus. Ceci se manifeste dans une tentative un peu désespérée pour certains, d’utiliser et de désigner ce qui est de la lexicométrie par de nouveaux noms (lexicologie, analyse textuelle assistée par ordinateur[39])  de façon à masquer le caractère lexicométrique de leurs travaux, et d’être enfin reçus et compris de leurs confrères. Ce déni pose une question essentielle : pourquoi, alors que les méthodes informatiques d’analyse du discours commencent à être particulièrement performantes, les historiens continuent-ils à les rejeter ? Je crois que deux pistes pourraient être privilégiées. La première serait celle d’une résistance globale des historiens français d’accéder à une formation en informatique. Le cursus universitaire en serait un symptôme[40]. L’informatique est très souvent délaissée au profit de matières considérées comme plus nobles. Les historiens pensent qu’il est inutile d’acquérir ces outils pourtant devenus indispensables. Cette inertie serait alors à penser comme une exception française, ce que la comparaison avec l’Allemagne et le monde anglo-saxon tendrait à prouver. Les théories de l’information et l’utilisation de la lexicométrie y sont beaucoup plus répandues. La seconde serait à trouver dans un refus, de la part des historiens, de changer leurs habitudes de recherche, fondées sur un modèle hypothético-déductif[41]. La plupart du temps, les chercheurs en histoire interrogent leurs sources à partir des questions issues d’une problématique. Les utilisateurs de la lexicométrie  mettent à distance cet habitus, puisque, s’ils ne rejettent en aucune façon la formulation d’une problématique[42], ils choisissent de ne pas se prononcer à l’avance sur ce que nous dit une source, mais de laisser cette dernière « parler d’elle-même » par le résultat statistique. Le principe est en quelque sorte de se laisser surprendre par ce dernier et de l’analyser en fonction des problématiques que l’on a bâties, ou de réviser ces hypothèses. Cette méthode ne résout pas le problème de l’interprétation nécessairement subjective de l’historien, puisqu’aucun chiffre n’a de sens en lui-même et qu’il doit obligatoirement être interprété.  Cependant elle recule le moment de l’interprétation en nous permettant, grâce aux résultats bruts, de nous abstraire de nos préjugés sur la source et de nos désirs de résultats. Elle est un outil heuristique d’une très grande qualité. Elle permet aussi de solutionner le problème récurrent du statut de la citation en histoire, comme le montre Antoine Prost. Grâce aux outils d’aide à la sélection de la citation, celles que l’historien utilise pour fonder son propos ne sont pas le fruit d’une préférence personnelle, mais bien une citation véritablement représentative d’un corpus. L’usage de la lexicométrie peut être extrêmement bénéfique puisqu’il met fin à certains problèmes. Pour que les résultats statistiques aient un sens et soient recevables, le corpus doit suivre des règles strictes de composition, que je détaillerai dans un prochain billet. * La lexicométrie a donc connu trois phases de développement. Elle est née des deux courants intellectuels majeurs des années 1960 : le structuralisme et le quantitativisme. Ses résultats ont très vite été contestés puisqu’elle utilisait des mots pivots, fétichisait le chiffre et se rendait coupable d’anachronisme. Il semble cependant que l’on ait « jeté le bébé avec l’eau du bain », puisque le rejet brutal de ces méthodes n’a pas tenu compte de leur renouveau et du développement de logiciels qui avaient intégré les critiques que leur avaient faites leurs opposants. Ce rejet est sans doute la manifestation d’une crainte plus profonde, d’une remise en cause plus globale des travaux qui n’utilisent pas ces méthodes. Il est très regrettable que ces méthodes extrêmement efficaces demeurent si marginales dans notre pratique de l’histoire, alors qu’elles pourraient constituer un complément très intéressant à la lecture non médiatisée par le recours à l’ordinateur. J’insiste ici sur le terme de complément : les historiens qui utilisent la lexicométrie n’ont pas pour but de remettre en question les résultats de ceux qui ne l’utilisent pas, mais bien de proposer une méthode renouvelée et efficace qui pourrait apporter de nouvelles perspectives de recherches. Je conclurai en citant l’appel lancé par Antoine Prost à ses collègues (désignés par « les premiers) : « Les premiers sont en France les plus nombreux, et l’on désespère de le convaincre, à moins d’une tentative loyale de leur part, comme celle que nous leur proposons. Qu’ils commencent, comme nous l’avons fait nous-mêmes, par lire les textes sans appareil statistique, et qu’ils consignent par écrit leurs remarques. Qu’ils procèdent ensuite à une seconde lecture, en se pliant cette fois aux contraintes quantitatives : ils feront des remarques qualitatives qu’ils n’auraient même pas soupçonnées quand ils refusaient les servitudes du nombre. »[43] . .    [1] Claire Lemercier et Claire Zalc, Méthodes quantitatives pour l'historien, Paris, La Découverte, 2008, 120 p.  [2] J’ai assisté à l’atelier de Claire Lemercier et Claire Zalc, l’histoire et l’historien-ne face au quantitatif. Pour plus d’informations sur ce séminaire, voir : http://www.quanti.ihmc.ens.fr/Atelier-2011-2012.html.  [3] J’ai suivi le TD de Benjamin Deruelle sur l’histoire et l’informatique à Paris 1. Ses cours sont disponibles sur l’ENT de la Sorbonne à l’adresse : http://tdhist.univ-paris1.fr/(uniquement pour les étudiants de Paris 1)  [4] Cette question de la prise en compte de la forme même du texte est très bien exposée dans le livre de Dinah Ribard et Judith Lyon-Caen, L’historien et la littérature, Paris, La découverte, 122.p. Cet objectif est défini en introduction : « Le présent ouvrage se situe résolument du point de vue de l’histoire, mais d’une histoire qui ferait de la littérature non pas un réservoir de sources, mais son objet même. » p.5. Sur Devenir historien-ne on pourra consulter le billet consacré aux usages historiens de la littérature, par Émilie Gimenez et Maxim Martignon.  [5] Ce billet, nécessairement synthétique, ne prétend pas à l’exhaustivité, notamment en ce qui concerne le structuralisme, la linguistique et le lacanisme.  [6] Oswald Ducrot, Qu’est-ce que le structuralisme ? 1. Le structuralisme en linguistique, Paris, Seuils,1968, 123 p.  [7] Idées exprimées successivement par Rousseau et Locke. Jean-Jacques Rousseau, Les Confessions, Paris, Gallimard, 2009, 858 p. et John Locke, Essai sur l’entendement humain, Paris, Vrin, 2002, 640 p.  [8] Entre autres livres : Claude Lévi-Strauss, Tristes Tropiques, Paris, Pocket, 2001, 513 p.  [9] Ce concept d’homoncule est mis en évidence par Irène Théry dans son commentaire de la philosophie individualiste. Voir Irène Théry, Des humains comme les autres, bioéthique, anonymat et genre du don, Paris, EPHE, 2010, 309 p.  [10] Idée de Paratopie qui définit l’auteur est définie par Dominique Maingueneau dans Le Discours littéraire, paratopie et scène d’énonciation, Paris, Armand Colin, 2004, 261 p.  [11] Ferdinand de Saussure, Cour de linguistique générale, Paris, Payot, 2005, 520 p. Saussure a développé ces idées de structures bien avant Lévi-Strauss puisqu’il est mort en 1913. Cependant il n’a pas eu d’écho à son époque et ce n’est que grâce à Jackobson et à Lacan, qui ont redécouvert ses théories et les ont diffusées que ses idées ont été reconnues.  [12] Platon, Le Cratyle, Paris, Garnier Flammarion, 1999, 317 p.  [13] Aphorisme lacanien expliqué par Michel Arrivé dans Linguistique et psychanalyse, Freud, Saussure, Hjelmslev, Lacan et les autres, Paris, Meridiens Klincksieck, 1987,180 p.  [14] Antoine Prost dans « Les mots » (in René Rémond, Pour une histoire politique, Paris, Seuil, 1988, pp. 255-287) souligne lui aussi ce passage. « L’affirmation, dans les années 60 d’une linguistique bien différente de la philologie qui en tenait lieu jusqu’alors, a constitué un tournant majeur. » p.255.  [15] Oswald Ducrot, Qu’est-ce que le structuralisme..., op. cit., p.18.  [16] Le manifeste du cercle de Vienne exprime clairement cette idée, comme le rappelle François Rastier, Arts et sciences du texte, Paris, PUF, 2001, 303 p.  [17] Claude Lévi Strauss nous explique dans Tristes Tropiques (op.cit, chapitre XX) que les sociétés sont composées, tout comme les objets qui nous entourent de différents éléments que l’on peut recenser tout comme l’avait fait Mendeleiev dans sa table et que son objectif est de trouver le « tableau périodique des sociétés ».  [18] Dominique Maingueneau, L’analyse du discours, introduction aux lectures de l’archive, Paris, Hachette supérieur, 1991, p.13.  [19] Christian Delacroix, François Dosse, Patrick Garcia, Les courants historiques en France XIXe-XXe siècles, Paris, Gallimard, Folio Histoire, 2005, 724 p.  [20] Cité par Claire Lemercier et Claire Zalc, Méthodes quantitatives…, op.cit, p.9.  [21] Ibidem.  [22] Méthode inventée par Bloomfield dans les années 1930. Consiste à tenter de comprendre et de quantifier les différentes positions que prennent les syntagmes dans une phrase. Sur ce point, voir les deux sites suivants : http://www.larousse.fr/encyclopedie/nom-commun-autre/distributionnel/44076#328607 et http://www.universalis.fr/encyclopedie/distributionnalisme/2-le-debat-avec-la-linguistique-generative/  [23] Régine Robin, Histoire et linguistique, Paris, Armand Colin, 1973, 306 p.  [24] Ibidem, p.16.  [25] Ibid.  [26] Antoine Prost, Vocabulaire des proclamations électorales de 1881, 1885 et 1889, Paris, PUF, 1974, p.6.  [27] C’est le groupe de Fontenay qui s’en occupe prioritairement. Voici une liste des principaux contributeurs des deux groupes ainsi que leurs successeurs avec le type de travaux qu’ils ont menés : Jean Dubois (directeur de l’équipe de l’ENS St Cloud), Antoine Prost (propose la première étude fondée sur la lexicométrie grâce au Vocabulaire..., op. cit.), Louis Girard, Jean-Philippe Genet (historien du Moyen Âge et de la construction de l’État moderne), Ludovic Lebart (polytechnicien qui a travaillé avec André Salem pour créer le logiciel lexico3), André Salem (linguiste),  Jean-Pierre Faye, Régine Robin (historienne et sociologue, étudie les discours des contre-révolutionnaires), Jacques Guilhaumou (avec Régine Robin, travaille essentiellement sur l’analyse de l’événement à l’ENS de St Cloud), Denise Maldindier (participe avec Guilhaumou dans les années 70 à l’analyse de discours. Son travail de thèse porte sur les discours de la guerre d’Algérie),  Denis Peschanski (analyse le vocabulaire de l’Humanité entre 1934 et 1936), Alphonse Dupront (analyse le vocabulaire des cahiers de doléance).  [28] François Rastier, Arts et sciences du texte, Paris, PUF, 2001, 303 p.  [29] Viallaneix, trouve, dans l’étude de l’œuvre de Michelet, des couples de notions telles que peuple/race, peuple/patrie, peuple/révolution et oublie de justifier ces choix purement personnels. Le concept de mot pivot est expliqué par Jacques Guilhaumou, Discours et événement, op.cit, p.15.  [30] Antoine Prost, un historien spécialiste de la lexicométrie, souligne ce problème : « Une fréquence est un fait brut, sans signification par lui-même. Dans les proclamations de 1881, (…), le terme France est employé 60 fois sur 10.000 mots de texte. Que pouvons-nous tirer de cette constatation ? Rien ». Antoine ProstAntoine Prost, Le vocabulaire..., op.cit, p.14.  [31] Régine Robin, pourtant partisane de la lexicométrie, met en évidence cette dérive : «  En réalité on peut faire dire à l’appareil statistique n’importe quoi » in Histoire et linguistique, Paris, Armand Colin, 1973, p.16  [32]Christian Delacroix, François Dosse, Patrick Garcia, Les courants historiques en France XIXe-XXe siècles, Paris, Gallimard, Folio Histoire, 2005, 724 p.  [33] Logiciel conçu par Max Reinert dans les années 90. Pour en savoir plus voir : http://www.image-zafar.com/index_alceste.htm.  [34] Logiciel créé dans les années 80 pour mettre les textes de la Révolution à disposition. Voir http://www.unice.fr/bcl/spip.php?rubrique38.  [35] Damon Mayaffre, Paroles de président : Jacques Chirac et le discours présidentiel sous la Ve République, Paris, Champion, 2004, 291 p.  [36] L’Analyse Facorielle des Correspondances est une méthode de visualisation des répartitions des mots dans un corpus. J’y reviendrai plus précisément dans un prochain billet voir : voir http://www.quanti.ihmc.ens.fr/-Analyse-factorielle-.html et Philippe Cibois, L’Analyse factorielle, analyse en composantes principales et analyse des correspondances, Paris, PUF,1983, 126 p.  [37]Benjamin Deruelle, « Enjeux politiques et sociaux de la culture chevaleresque au XVIe siècle : les prologues de chansons de geste imprimées », Revue historique, n°655, 2010, pp.551-576.  [38] Tiago Mata, et Claire Lemercier, , « Speaking in Tongues, a Text Analysis of Economic Opinion at Newsweek, 1975-2007 »,  Duke University Center for the History of Political Economy Working Paper, n°2,  2011, http://ssrn.com/abstract=1753164, lien valide le 29/11/11.  [39] Dominique Maingueneau, L’analyse du discours, introduction aux lectures de l’archive, Paris, Hachette supérieur, 1991,268 p.  [40] Sur ce point voir les réflexions proposées par Émilien Ruiz dans "La boite à outils des historiens : (in)formations numériques pour les jeunes chercheurs", Les aspects concrets de la thèse, 9 juin 2011 : http://act.hypotheses.org/1195 et "Les historiens seront-ils finalement programmeurs ?", La boîte à outils des historiens, 22 sept. 2011 : http://www.boiteaoutils.info/2011/09/les-historiens-seront-ils-finalement.html.  [41] J’ai compris ces seconds éléments d’explication grâce à l’intervention de Benjamin Deruelle à l'occasion d'une conférence sur les usages de la lexicométrie en histoire lors de la 3e édition des "outils informatiques pour les historiens" qui s'est tenue à l’EHESS le 25 octobre 2011 (voir le compte rendu sur La boîte à outils des historiens : http://www.boiteaoutils.info/2011/11/outils-informatiques-pour-les-historien.html)  [42] Aucun des historiens que je cite dans ce billet ne remet en question l’histoire problématisée.  [43] Antoine Prost, Vocabulaire..., op.cit, p.14.