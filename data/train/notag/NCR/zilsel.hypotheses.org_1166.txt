Le sociologue des sciences en formation continue qui, comme moi, trouve un certain charme à Leiden, ne pouvait qu’être inspiré par la publicité postée sur leur site annonçant des places toujours disponibles pour la formation « Measuring Science and Research Performance » proposée du 8 au 12 septembre 2014. Bon, j’apprendrais par la suite que le contingent habituel de fonctionnaires coréens était exceptionnellement réduit pour cause de vacances, ce qui peut expliquer les places restantes. Ni le prix (je laisse au lecteur le plaisir de chercher l’information sur le site) ni la seconde partie du titre du cours – ah, la performance de la recherche ! – ne m’ont détourné d’y participer. Il faut dire que le CWTS (Centre for Science and Technology Studies) est le must en termes de scientométrie (« this means studying scientific and academic research from a scientific point of view »[1]). Ils ont des énormes bases pour lesquelles des chercheurs viennent de toute l’Europe ; ils conçoivent des logiciels que tout bon scientométricien est censé connaître (VOSviewer) ; ils publient dans Scientometrics, Research Policy et autres revues spécialisées. En bref, ils sont visiblement bons dans leur domaine et sollicités par les agences nationales et européennes pour la production d’indicateurs sur la science. D’ailleurs, cette année ils fêtent leur quart de siècle et affichent fièrement « 25 years of meaningful metrics ».  Meaningful logo (crédits : CWTS) Mais pourquoi parler de ce cours ? Les conférences, à la limite, relèvent d’une actualité pertinente pour la recherche. Quel intérêt de venir vous conter par le menu une formation professionnelle ? Déjà parce que le public visé est – aussi – un peu les chercheurs impliqués dans la science policy. Mais surtout parce que cela relève d’un début d’observation participante d’un phénomène qui, aussi contesté peut-il être, n’en est pas moins d’une actualité brûlante. Qu'il s'agisse de la formalisation, de la diffusion ou de l’utilisation des indicateurs de mesure de la science, sans compter la mise en logiciels, méthodes et livres des métriques de l’innovation au service d’acteurs stratégiques cherchant la réflexivité sur leurs pratiques : c’est peu dire que la question est d’actualité, comme on dit dans les gazettes[2]. Et ce n’est pas moi qui vous le dis : c’est l’autre chercheuse allemande réalisant une thèse sur l’institutionnalisation de la scientométrie qui semble aussi jouer ce jeu trouble à mi-chemin entre observation participante et formation. Mesurer la science, certes, mais au service de la « performance ». Science de la science, certes encore, mais au service des politiques scientifiques. Que nous promet donc cette formation ? Nous allons recevoir des exposés sur l’analyse des citations, l’exploitation de l’analyse des réseaux et des cartographies de la science. Nous allons être initiés à l’impact « sociétal » de la recherche. Mais comme on n’apprend jamais mieux qu’en mettant la main à la pâte, les « apprenants » conduiront un travail tout au long de la semaine à travers une série de « travaux pratiques » et réaliseront ainsi en petits groupes une évaluation de la performance d’une université. Ça fait chic, n’est-ce pas. À notre service, des indicateurs bibliométriques avancés (je souligne), les outils « maison » de cartographie, des données en voulez-vous en voilà… Tout ce qu’il faut pour faire une évaluation, quoi. Sauf le montant du budget recherche des universités et le nombre de leurs ressources humaines. Ça c’est bon pour le chercheur has been. Avant que l’on ne me taxe d’une quelconque intransigeance critique dans mon évaluation du cours, je voudrais préciser un point qui aura son importance par la suite : je ne cherche pas à sous-entendre quoi que ce soit quant à un déficit d’expertise du CWTS ou de qualité des intervenants. Au contraire, et que l’on soit clair, j’ai trouvé les outils existants et les études de cas présentées vraiment intéressants. Les enseignants-démonstrateurs étaient précautionneux dans leurs affirmations, ils ont abordé des sujets d’actualité de la recherche en scientométrie sans passer sous silence les discussions actuelles. Ce dont il sera question dans cette restitution presque instantanée, ce sera  plutôt le format même du cours, ce qu’il représente et ce qu’il entend transmettre. Ce qui ne laisse pas de m’interroger, c’est l’existence et la signification mêmes de ce type d’enseignement dans le contexte actuel de la dite « économie de la connaissance »[3] et de son actualisation dans les universités sous le sceau de l’« autonomie ».  Old-school ISI (crédits : website of the Institute for the Study of Urologic Diseases) Déjà, un peu de contexte. Qui assiste à un tel cours ? Nous ne sommes pas nombreux, je suis le seul français. A part ma camarade doctorante, les autres participants ne sont pas universitaires. Sur les vingt participants (effectif restreint souhaité pour permettre de travailler en petits groupes avec nos tuteurs) : onze personnes sont rattachées aux services des bibliothèques ; trois sont chercheurs (dont deux PhD students) ; une ingénieure d’Elsevier, elle, participe au développement des outils de la base bibliographique Scopus ; on compte aussi un agent de l’administration de la recherche coréenne et quatre experts « senior » qui conseillent des universités, dont trois russes. Donc en gros, un public très internationalisé et orienté vers les sciences de l’information et de la bibliothèque. Ce n’est pas si surprenant, car la scientométrie a fait ses premières armes en répondant aux problématiques des bibliothécaires à l’époque glorieuse du International Citation Index (ISI) de Eugene Garfield, lesquels bibliothécaires avaient à choisir les revues intéressantes auxquelles s’abonner[4]. Ainsi ce n’est qu’un juste retour aux sources – les BU sont des clientèles captives et captivées. Cependant, cela donne déjà le ton du type de problématique qui sera traité : une partie de ces personnes sont en situation de fournir un support aux chercheurs de leurs établissements mais aussi à informer les décisions stratégiques dans un domaine bien précis. Parce que le Carnet Zilsel a aussi pour ambition d’apporter des informations sérieuses à ses lecteurs (qui en douterait, d’ailleurs ?), je vous propose un  résumé de cette semaine de cours, dans la rubrique « Conference advisor ». Une question lancinante guidera le texte : que doit-on savoir et espérer quand on veut mesurer la science dans le but de produire une évaluation de la performance des universités ?[5] Jour 1 : collecter les données Comme le veut la tradition universitaire, la conférence d’ouverture est donnée par le directeur des lieux, Paul Wouters. Elle s’avère de loin la contribution la plus sociologique de la semaine et souligne (pour la forme ?) de nombreux enjeux et limites de la scientométrie. On ne pourra donc pas dire qu’on ne le savait pas. Au passage, le directeur promeut le CWTS comme source d’expertise. À partir de la couverture de The Economist, qui titrait en 2013 « How science goes wrong », Wouters amène rapidement la nécessité impérieuse de nouveaux moyens de garantir la qualité de l’activité scientifique et de pallier la déficience des formes de régulation traditionnelles de l’institution scientifique, particulièrement aiguë dans les sciences biomédicales. D’ailleurs, souligne-t-il, il s’agirait de la raison d’être du CWTS : en effet, prévenir l’écart qui se creuserait entre les critères d’évaluation, les fonctions sociales et économiques de « la science », le changement d’échelle et les nouveaux rôles des chercheurs (tout ça à la fois, donc). Une des solutions proposées consiste à utiliser les citations d’articles afin d’envisager plus efficacement l’« impact » des productions scientifiques, et de développer ainsi un panorama des théories de la citation. Ces théories de la citation auront un air de déjà vu pour le sociologue des sciences : elles suivent les contours des débats maintenant classiques du domaine en distinguant des conceptions fonctionnalistes (les citations utiles à l’argumentation des conceptions normatives), les citations comme reconnaissance d’une antériorité, et les autres registres d’action stratégique (argument d’autorité, rhétorique, etc.). Assurément, ces théories de la citation sont imparfaites, car citer l’article de quelqu’un peut s’expliquer par de nombreuses raisons que la raison scientométrique pourrait méconnaître. Pourtant, ce n’est pas le plus grand problème. Le danger, le grand souci même, réside dans la dimension performative des théories de la citation : le monde de la recherche n’est pas découplé des indicateurs, et de nombreuses rétroactions existent qui complexifient le système[6] . Car en effet, les chercheurs participent à la création des indicateurs, ces mêmes indicateurs sont utilisés pour définir des politiques scientifiques qui redessinent les contours des institutions, lesquelles finissent par agir sur les chercheurs. Le cycle de la publication scientifique et le cycle de la scientométrie ne sont donc pas dans un rapport d’autonomie, et les différentes formes d’hybridation entre expertise par peer-review et utilisation d’indicateurs bibliométriques rendent difficile l’identification et la formalisation d'une norme claire de « bonne pratique ». Cette zone du « black hole in ‘informed peer review’ » ne bénéficie en outre d’aucune systématicité : « you need to be pragmatic […] and you have to justifiy what you’re doing in every case study ». Un sage conseil, dans un monde où l’usage d’une multitude d’indicateurs plus ou moins explicités se généralise pour constituer autant de « machines d’évaluation » (des « machins », serait-on plutôt tenté de dire). Rendre la recherche « meilleure » en se donnant la possibilité d’une véritable évaluation, voilà donc ce que serait la mission du CWTS. L’intervenant développera un peu plus la question de l’histoire des indicateurs, leurs usages dans différents exercices d’évaluation de la recherche. Et l’on en apprendra un peu plus sur les clients (sans guillemets !) du CWTS : les gouvernements nationaux, les universités, les agences de recherche… La mission que nous devons développer tout au long du cours, on n’aura de cesse de s’y confronter : l’évaluation de la performance d’une université. Cela commence à entrer dans les crânes.  Corporate mug (crédits : Leiden University) Pause déjeuner. Il faut savoir qu’aux Pays-Bas, le repas du midi n’est pas un vrai repas (tropisme national). Toute la semaine, nous aurons droit à un bol de soupe et quelques sandwichs. Ajoutée à cette austérité culinaire, l’inertie du premier moment de socialisation entre des personnes d’horizons professionnels si divers, ce n’est pas l’ambiance des grands soirs, loin de là. On se tient donc entre « apprenants », répartis sur les petites tables d’un hall impersonnel où une conversation aura bien du mal à s’établir. Cela ira un peu mieux les fois suivantes, mais la dynamique interactionnelle restera limitée et peu propice aux connivences tout au long du cours. Reprise pour une après-midi dédiée aux sources de données. Le saint Graal qui nous est présenté est la base de données nettoyées et vérifiées contenant toutes les publications de l’institution. Le microscope du scientométricien n’aurait plus qu’à être actionné. Si l'on y apprend bien quelques « trucs » que le CWTS met au service de ses clients (première moisson d’articles, puis vérification de ces publications par les chercheurs à travers une interface déployée pour l’occasion), on en arrive vite à une triste conclusion : les données, c’est le Web Of Science (WOS). Alors, certes, le WOS couplé à d’autres bases de données quand cela est nécessaire, et un peu nettoyé à l’occasion, mais cela ne va pas beaucoup plus loin. C’est un peu décevant pour qui l'utilise déjà, mais bon, il est vrai que le nettoyage, la clarification, l’agrégation des données du WOS pour en faire quelque chose d’utilisable sérieusement – et pas uniquement afficher des résultats aléatoires – est un travail de fourmi, très laborieux, et on imagine que c’est là que réside probablement la très grande valeur ajoutée du travail fourni par le CWTS. Mais néanmoins, cela  ne fait que limiter, et non pas résoudre, les éternels problèmes d’homonymie (J. Smith et L. Chen ont beaucoup, mais beaucoup d’articles à leur nom !), de sources non référencées (bonjour les SHS et leurs livres sans aucune valeur, puisque sans impact factor) et la polysémie des institutions (comment ça, des gens rajouteraient « Sorbonne » au titre de leur université parce que ça fait mieux ?!). Parce que s’il existe d’autres sources (Scopus, plus récent que le WOS, et Google Scholar), le premier semble souffrir de défauts cachés et le second est une énorme boîte noire non maîtrisée. D’ailleurs, un des travaux réguliers de la recherche en scientométrie est de faire le point comparatif entre la couverture de ces bases de données bibliographiques, confrontées à une très grande volatilité au gré des politiques suivies par leurs administrateurs. Que reste-t-il comme solution, dès lors : tenter d’estimer le taux de couverture, et constater que si le WOS est excellent pour les sciences biomédicales (bah tiens !), les SHS sont encore et toujours les grandes sacrifiées de l’affaire. Tant pis, on ne s’y intéresse pas trop de toute façon quand on parle de performance-des-universités. Ce n’est pas comme si elles avaient un intérêt économique, après tout... Morale de cette après-midi : c’est compliqué un CV de chercheur, et puis ils ne sont même pas fichus de mettre le titre exact de leur publications sur les pages web… Ils pourraient penser un peu plus aux scientométriciens, bon sang ! Pour cela, on nous a demandé de faire une recherche exhaustive du taux de couverture des bases de données pour deux CV de chercheurs. « Nétwayé, baléyé, astiké… Kaz la toujou penpan » (Nettoyer, balayer, astiquer… La maison est toujours propre, célèbre refrain de Zouk Machine), chante-t-on à tue-tête en bidouillant les bases. Nul doute que mon après-midi n’aurait pu être mieux employée. À la fin, on a effectivement envie d’édicter des règles bien strictes de complétion de CV, que des managers pourraient imposer aux chercheurs. Me voilà un bon petit entrepreneur de la normalisation. On me souffle dans l’oreillette que c’est déjà le cas dans de nombreuses universités, conditionnées par l’allocation de financements, donc je suis à la page. Jour 2 : Analyse des citations Nouvelle couche sur les théories de la citation : que veut dire citer quelqu’un ? Différence entre citation et référence ; opposition entre une lecture fonctionnaliste (la citation comme reconnaissance d’un travail antérieur) et une lecture stratégique (la citation comme élément rhétorique dans un processus de persuasion). Mais aussi, de quoi la citation est-elle alors le proxy ? La scientométrie essaie depuis longtemps de justifier leurs usages à grand coup d’étude de corrélations entre l’avis des experts et les indicateurs scientométriques. Mais quelle solution est apportée pour donner un sens à de tels indicateurs ? Rassurez-vous, ils sont normalisés par domaine scientifique, histoire d'y voir plus clair et de sérier. Mais attendez, me diriez-vous, normaliser un indicateur ne le justifie pas, ou permet à la rigueur de le rendre comparable d’un domaine à l’autre. Et encore, en posant cette question, on oublierait le fait qu’on n’a pas encore de réponse définitive (et ce n’est pas faute d’avoir essayé...) à ce que serait un domaine scientifique. Et c’est là que le premier grand écueil de la scientométrie se révèle de la manière la plus flagrante : la tension qui existe entre la reconnaissance lucide d’une ambivalence dans les interprétations que l’on peut donner aux indicateurs et pourtant la grande rigueur technique mise en œuvre (normalisation par domaine ; fenêtrage temporel ; grandeurs intensives ; suppression des autocitations, etc.). Dans cette explosion de choix techniques, tous les indicateurs ne se valent pas, et le trop célèbre index de Hirsh, mieux connu sous son nom de « h-index », n’est peut-être pas si bien que ça, car « we think there is better indicators at CWTS ». Il faut reconnaître une certaine sobriété dans les indicateurs utilisés par le Centre, garants d’une maîtrise pragmatique manifestement empreinte de sagesse : le nombre total de publications P, de citations C, et puis le nombre moyen normalisé de citations par article MNCS (Mean normalized citation score), le même au niveau des journaux MNJS (Mean normalized journal score) et le très intéressant indicateur de proportion des publications dans le top 10 % PPtop (on pensera par exemple à l’usage très intéressant que font les économistes comme Thomas Piketty du décile, du centile et autres fractions dans l’analyse des inégalités économiques). Ces indicateurs nécessitent de connaître la répartition de l’ensemble des publications et donc d’avoir accès à une base extensive introduisant de potentiels biais de couverture. Mais au final, comme le reconnaît le conférencier entre deux phrases : « we do what the client wants in the study ». Je crois que tout est dit : un gouffre béant existe entre la réflexivité assez poussée qu’ont les chercheurs du laboratoire au sujet de la fabrication des indicateurs et ce qu’au final les clients en font. Cette conscience des limites des indicateurs est d’ailleurs intensivement présentée dans le cas de l’impact facteur et du h-index sans pour autant être étendue aux indicateurs « maison ». La réflexivité n'est donc pas sans limite lorsque les scientométriciens en viennent à agir. Agrégés à un niveau supérieur, ces indicateurs permettent d’élaborer des classements, lesquels créent les conditions d’un benchmarking permanent[7]. La profusion de ces classements et l’importance croissante qu’ils se voient accorder dans les médias et dans certains milieux des politiques scientifiques sont aussi très visibles aujourd'hui[8]. Quatre hit-parades sont approfondis : le « classement de Shanghai » forcément, le Times Higher Education University Ranking, le QS University ranking et le classement « maison » du CWTS (remarque en passant : étonnante coïncidence, c’est seulement dans le classement du CWTS que Leiden University est la première université des Pays-Bas). Bien sûr, le classement de nos hôtes est le meilleur, il est bien plus rigoureux et plus transparent. Sa création est motivée par la volonté de reprendre la main sur le type d’indicateurs utilisé pour éviter les grandes salades de fruits type « Shanghai » (on vous mettra bien un quart de Nobel et deux tiers de CNRS ?) : « The broad fields have been defined at the level of individual publications rather than at the journal level. Using a computer algorithm, each publication in the Web of Science database has been assigned to one of these seven fields. This has been done based on a large-scale analysis of hundreds of millions of citation relations between publications. » Salutaire conclusion sur les critiques de premier degré que l’on peut faire à ce genre de classement : ils entretiennent la rhétorique de la rentabilité à l’investissement du contribuable et ils donnent une image erronée de la comparabilité des systèmes de recherche.  Jours 3 : Analyse de réseaux de citations Ah, l’analyse de réseau bibliométrique. Jusqu’à présent, il manquait ces magnifiques réseaux aux couleurs vives qui attirent le regard, rendent guilleret le décideur et soulage le chercheur d’une véritable analyse en se substituant, le temps d'une communication perfusée sous PPT, à un sympathique présentateur météo. En vérité, je le confesse, j’adore l’analyse de réseau bibliométrique, je trouve qu’elle recèle un potentiel énorme, et c’est la raison pour laquelle je suis d’autant plus dur avec leurs usages faibles. Ludo Waltmann se charge de cette introduction. Il est le co-développeur de l’interface utilisée et disponible gratuitement, VOSViewer. Après avoir abordé tout ce qu’il est envisageable de faire à partir du moment où l’on accède à l’énorme masse de données que représente le WOS (réseau de co-citations, de publications par auteurs, par citation, de co-auteurs, de co-institutions et tous les dérivés envisageables en passant par l’analyse des termes présents dans les abstracts…), il lance quelques noms d’outils d’analyse avant de se concentrer sur sa progéniture.  Capture (crédits : VOSviewer) Si quelques minutes sont consacrées aux principes sous-jacents, il faut bien se rendre compte qu’une grande partie du traitement qu’il nous montrera – et que nous serons en mesure de faire dans les travaux pratiques – nécessite un prétraitement qui demeure hors de portée pour la très grande majorité des personnes dans l'assistance. Tromperie sur la marchandise ? En fait, après avoir posé la question à plusieurs d’entre eux, et malgré quelques irritations pour certains, ils n’ont pas l'ambition de le faire par eux-mêmes. Savoir que ça existe devrait leur suffire, et s'il fallait faire appel à ces outils dans un avenir proche, ils savent qu'il est possible de sous-traiter via le CWTS. Chacun son boulot, en somme. Pour mettre l’eau à la bouche – je dois préciser que c’est efficace ! –, il nous présente une étude de cas réalisée dans le cadre justement d’un de ces contrats pour lequel il a comparé la répartition de la production de connaissances pour différents hôpitaux d’une même université. C’est une « démo », au sens de Claude Rosental[9]. La superposition sur la carte globale des sciences extraite d’un traitement d’ensemble des 10 millions de publications du VOS et du réseau de références ainsi constitué des productions de chaque hôpital permet en un coup d’œil d’identifier les morphologies des productions scientifiques. Joli. Et utile. Même s’il faut bien prendre en compte qu’une visualisation n’est toujours qu’une représentation imparfaite. Et pour aller plus loin dans l’exploration d’un corpus d’articles, un autre logiciel « maison », aussi disponible, apparaît très utile : CitNetExplorer. C’était sûrement l’intervention technique la plus captivante car elle a fait le lien entre les données, les logiciels de traitement et différents niveaux d’analyse. Et pour avoir l’occasion d’utiliser les données produites par Ludo Waltmann, qu’il met à sa disposition sur sa page internet, je dois reconnaître que ces technologies, bien qu’elles incorporent un ensemble de choix qui détermine largement le rapport à l’objet, permettent un intéressant changement d’échelle dans la réflexion en sociologie des sciences. Elles donnent l’occasion de poser des questions à l’échelle globale et de mettre en contexte des études plus locales. Si cartographier la science vous intéresse, je vous invite aussi à jeter un coup d’œil au cours de Katy Borner. En résumé, les cartes c’est sympathique et édifiant, et la prouesse technique est belle en soi (faire une carte du savoir !). Les discussions commencent vraiment – mais on peut dire qu’il en va de même pour la bombe nucléaire ou de n’importe quelle technologie potentiellement incontrôlable – au moment de leur usage. Et le CWTS dans ses études contractuelles utilise ce type de représentation pour « créer du contexte aux institutions ». J’en ai déjà parlé rapidement : cela signifie placer la production d’une organisation par rapport à la toile constituée par l’ensemble des publications existantes. L’innovation est d’aller au-delà d’une cartographie sommaire des différents journaux. En effet, il y a le cas de Science et Nature, au sujet desquels « you can say they are not journal ». Le CWTS propose l’utilisation d’une carte bottom-up des zones de forte densité de co-références entre articles. Cette carte, qualifiée « d’objective » (heureusement que nous avons toute une palette d’objectivités à notre disposition[10]) est composée au second niveau de hiérarchie de 784 champs empiriquement constitués permettant de dresser le profil des universités. La force, vous l’aurez peut-être compris, réside dans la possibilité de normaliser les productions par champs et donc de donner des valeurs qui peuvent prétendre à plus de sens qu’une valeur brute. Et pour quoi faire, me direz-vous ? Je laisse les profs le justifier : « You can use the map to better caracterize the impact. [...] But most of the time it comes from general analysis of the university, or other, and we use this map to better illustrate what we see, get a better vision ». Donc, en gros, comme solution d’appoint. « The map that we show are not some kind truth, it’s a representation of your data. It’s never perfect … it says something, but it’s not like a truth. That’s a kind of … a bit more fake. That’s why people distrust it a little bit. Support with other analysis. ».  Tout est dit. Cela n’enlève rien à l’intérêt énorme de ce type d’outil. Il doit cependant être soutenu par un savoir expert et une connaissance des logiques historiques, sociales et politiques à l’œuvre. Et c’est là que, il faut bien le dire un moment ou à un autre, la sociologie des sciences et des techniques arme utilement l’analyse. Jour 4. L’« impact sociétal » Le titre de la journée est déjà problématique : il est composé de deux mots dont chacun fait crisser les dents du sociologue. L’« impact », qui plaît tellement aux décideurs, et le « sociétal », ce pseudo-social flanqué de son disgracieux suffixe, source de toutes les attentions. Le postulat de base, surtout issu des acteurs en situation de décision, est le suivant : « comme la recherche est devenue importante pour l’économie et la société, il faut pouvoir le mesurer ». Rien de nouveau sous le soleil : la croissance des indicateurs au service des formes de gouvernement des activités, encore et toujours. Ce qui l’est plus est la forme spécifique que cela prend pour l’activité de la recherche. Le constat de l’autonomie relative du champ scientifique et la grande spécificité de la scientométrie classique s’accordent mal avec les intérêts des stakeholders et les processus d’innovation. Il faut d’autres métriques pour mesurer l’innovation, l’intérêt social et la diffusion des productions scientifiques. En réalité, il existe toute une variété de frameworks permettant de décomposer temporellement et analytiquement (ah, la fameuse échelle TRL) la notion d’« impact sociétal ». Une variante, sociologique à la louche et héritant des différentes études constructivistes des sciences et techniques, est celle des « interactions productives », laquelle propose de se concentrer sur toutes les formes d’interaction entre stakeholders, sources de création de valeur. Indépendamment de la base théorique privilégiée, le problème central reste très pragmatique : il n’existe aucune base de données unifiée comme cela peut être le cas pour la scientométrie. Et aucun accord sur le type de données, ni la manière de les coder. Il en résulte surtout une explosion des méthodologies qui semblent plutôt relever de l’accumulation d’études de cas, dont la qualité dépend de la sensibilité sociologique, économique ou autre du responsable. Deux aspects en particulier ressortent de l’exposé. D’une part, une très forte charge normative concernant l’injonction à communiquer vers la société : « If you don’t communicate quickly about your research, the chance it will have impact in long term is very low ». Et d’autre part, la tension très forte entre l’ethos du chercheur et ce que souhaiteraient les décideurs : « There’s no point in creating a score. But people like to have score for a lot of things. » Cela a donné lieu à une situation exemplaire suite à une question de l’audience sur le sens à donner à un graphique qui comparait pour différentes unités d’une université trois dimensions (scientifique, innovation, réputation, enfin quelque chose comme ça…). Après quelques échanges de part et d’autre, la conclusion était malheureusement que les valeurs représentées sur le graphique ne permettaient aucune comparabilité, et que seule la mise en contexte avait permis au responsable de l’étude d’en retirer des résultats. « You can put whatever weight you want. »  Steak holder (crédits : "vache hollandaise à Bruxelles", Reuters, via LaTribune.fr) Rapidement, nous avons aussi eu droit à une présentation des brevets, mais mis à part le caractère passablement procédurier et bureaucratique de la présentation de ce qu’est un brevet (il faut du courage pour se saisir des différentes législations nationales en vue d’une quelconque étude systématique…), le take away message était : construire une base de données exploitable est un enjeu énorme, hors de portée des pauvres mortels que vous êtes. Il semble qu’effectivement ce soit vrai. Les données sont complètement non normalisées, très largement sensibles à l’évolution historique des législations de chaque pays, et éparpillées entre les différentes organisations. Le concept de « famille de brevets » montre d’ailleurs à quel point il est important de reconstruire l’unité de l’analyse, qui n’est pas le brevet mais le groupe de brevets pris dans différents pays sous différents régimes administratifs pour protéger une unique innovation. Et puis, il faut bien se rendre compte : 60 % seulement des demandes conduisent à un brevet, et on ne parle pas de toutes les technologies gardées secrètes au sein des entreprises pour éviter leur dévoilement. Cette dimension pratique de la circulation des savoirs et des savoir-faire passe complètement en dessous des radars de telles analyses. Encore une fois, il existe un gouffre entre les traitements effectivement réalisés et la connaissance sur les conditions sociales de leur possibilité. Les anecdotes de telle entreprise qui dépose des brevets loin de son domaine d’expertise pour semer le doute parmi la concurrence ou celle du groupe qui joue sur les périodes de secret de différents services de brevets afin de prolonger la période de silence parlent d’elles-mêmes. Une dernière présentation fait un peu office de prophétie et de work in progress, tout en présentant pour le sociologue des sciences un tout nouveau champ à défricher : les altmetrics, ou métriques alternatives[11], sont les dernières arrivées dans ce joyeux champ de la scientométrie, version science 2.0. En substance, c’est la rencontre entre trois tendances assez différentes. Premièrement, il s’agit de la conclusion nécessaire du mode 2[12] de la recherche : la science ne produit plus sa valeur uniquement dans le champ scientifique mais à travers son « contact » constant avec la société, en répondant aux attentes des stakeholders. Il faudrait donc prendre en compte, pour évaluer la valeur d’une contribution dans ce mode-là, non seulement les publications, leurs citations, et éventuellement les brevets, mais aussi toutes les formes de reconnaissance : tweets, likes sur Facebook, blogs et tutti quanti. Deuxièmement, au sein même du champ scientifique, les logiques concurrentielles s’affirment de plus en plus et chacun est demandeur de données permettant de démontrer sa valeur : interviennent alors toute une flopée de nouveaux indicateurs égo-centrés : nombre de clicks sur l’article, nombre de personnes ayant téléchargé le Pdf, nombre de mentions sur des blogs ou sur des plates-formes dédiées comme Mendeley ou Academia. Troisièmement enfin, une dernière logique est celle du management de la recherche, qui n’en peut plus d’attendre des années pour j(a)uger de la qualité d’une production et veut se doter d’indicateurs à rétroaction rapide. Dès lors, quoi de mieux qu’une citation sur Twitter pour évaluer la « pénétration » d’un article dans la société ? Reste à savoir si tout cela n’est pas du vent emportant les petits oiseaux gazouillants de la recherche hype.  Google Trends, definitely trendy for business (source : WSI Global Reach) Donc les altmetrics c’est tout ça et rien que ça : des nouveaux indicateurs de l’espace public et des nouvelles technologies, généralement construits à l’échelle de l’individu ou de l’article (c’est interchangeable, d’ailleurs), qui sont mobilisés dans le cadre des ineffables « impacts sociétaux ». Ces indicateurs sont pour le moment utilisés à l’échelle individuelle et non systématique. Ils n’ont pas encore donné lieu à un investissement organisationnel plus poussé. Mais ça ne saurait tarder. Pour l’instant ils souffrent encore d’un certain nombre de limitations assez drastiques : la volatilité des sources et des outils (certaines plates-formes disparaissent d’un jour à l’autre…), l’impossibilité de normaliser ou de contrôler la qualité de l’information, etc. Mais le problème central est quand même le sens à donner : « The first problem is about the meaning … we don’t know what being tweeted means », concède Rodrigo Costas. Que veut dire une citation tweetée dans un monde où des bots peuvent être programmés pour tweeter automatiquement la sortie d’un nouvel article ? Néanmoins, cela semble un champ de recherche passionnant (je veux dire, comment objet d’enquête) et en pleine ébullition. À l’heure actuelle, le journal PLOS offre la palette la plus complète de ces indicateurs pour ses articles et le site altermetric.org met à disposition un ensemble d’outils pour « fouiller » le web sur tel article particulier. S’il semble très difficile d’être affirmatif sur ce que représentent de tels indicateurs (ce n’est pas parce qu’on est capable de calculer quelque chose que ce quelque chose devient automatiquement pertinent), ils permettent déjà de suivre les dynamiques d’ensemble de l’usage des savoirs scientifiques dans les médias. On voit ainsi apparaître un déficit très large des sciences physiques comparées aux domaines du biomédical. L’avenir est encore flou pour ce type d’information, mais beaucoup d’espoir y est placé quant à la possibilité de pouvoir mesurer cette utilité sociale de la connaissance. À suivre, à « fouiller » même. Jour 5. Restitution des travaux La présentation du travail en groupe a débouché naturellement sur ce qu’un tel cours pouvait produire. Il était temps en effet de nous engager dans le retour d’expérience. Le résultat est donc une manipulation plus ou moins fluide, de temps en temps inventive, souvent fausse, de productions chiffrées et autres cartographies. Entre l’absence de prise en compte des normalisations, l’évacuation de toute réflexivité sur la signification des indicateurs, les narrations s’écoulent mollement sur la performance de telle ou telle université. Rares sont les comparaisons entre établissements : les jugements de valeur se font en dehors de tout contexte. Telle université réussit bien dans la logique, ne publie pas assez, pourrait faire mieux... J’ai pour la forme placé un graphique donnant l’évolution du classement de Shanghai pour l’université dont je devais « rendre compte » ; graphique qui malgré la non-pertinence d’un tel indicateur n’a soulevé aucune question… Mais je suppose que ce silence s’expliquait par la lassitude et, surtout, la faim à l’approche du repas. Au passage, je trouve impressionnant la capacité de certains orateurs à donner une narration de la performance de leur université, dont je serais bien incapable (objecteur de conscience vis-à-vis de ce genre d’exercices, et à vrai dire peu doué pour les présentations en anglais, j’ai lâchement passé mon tour…). Pour ne prendre qu’un exemple des types de rationalité à l’œuvre et des biais heuristiques présents, je vais prendre celui dont j’ai été le témoin silencieux. L’université d’Uppsala produit une partie importante de ses publications dans la catégorie biomédicale (50 %). De fait elle apparaît en tête de tous les indicateurs (nombre de publications, publications normalisées, total de citations, etc.) Dans la discussion, on en vient vite à ne parler que de l’importance du biomédical (alors qu’aucun de nous n’a une idée du type de recherche qui se situe ci-dessous, ni du nombre de chercheurs, ni des budgets qui lui sont dévolus, comparé au misérable petit point des sciences humaines et sociales…), pour finir par conclure de l’importance de soutenir ce secteur. Tadam ! Nous avons nos recommandations prêtes. Ces heuristiques cognitives dont on reconnaîtra certains avatars décrits par l’économie et la sociologie cognitives conduisent facilement à des sillons narratifs hors sol. À l’inverse, sur le logiciel de visualisation VOSViewer, la cartographie représente par clusters deux informations normalisées : le nombre moyen de citations normalisé (MNCS) donné par une couleur allant du froid (bleu) au chaud (rouge), et l’importance du nombre de publications rapportée au champ représenté par la taille des bulles. Résultat, on est en mesure de voir les champs pour lesquels la proportion de publications de l’université est comparativement élevée par rapport à l’activité internationale et avec un taux moyen élevé des publications. Ce qui correspond souvent à des expertises très spécifiques liées à l’histoire de l’université et l’existence d’unités de recherche dédiées. Rien d’étonnant à ce que les rares chercheurs en logique ou en paléontologie ressortent ; en revanche il est plus dur, en tant qu’oncologue par exemple, de représenter, même à disposer d’une très grosse équipe, une partie significative de la production mondiale du domaine. Cette normalisation conduit à centrer le regard sur des éléments assez insignifiants (du point de vue de ces méthodes), ce qui n’a évidemment pas manqué. * *               * Je termine ce compte rendu sur les mêmes remarques que j’ai pu placer dans le questionnaire d’évaluation du cours : après usage, je trouve ce genre d’enseignement dangereux dès lors que, sous couvert de présenter différents instruments techniques neutres et objectifs, ses initiateurs ne prennent pas garde de les relier aux pratiques, ce qui est arrivé par moments comme je l’ai souligné. Indépendamment des aspects plus pragmatiques et assez subjectifs au demeurant (des cours trop légers, des travaux pratiques peu motivants ; mais à vrai dire je ne pense pas que je faisais partie du public visé…), ce sont les attentes mêmes du cours qui m’ont posé problème. Produire des narrations à l’échelle d’universités sur la « performance de la recherche » à partir d’indicateurs désincarnés et s’imposant partout selon la même logique unilatérale est le meilleur moyen pour convaincre (et fidéliser) largement les différents stakeholders du management de la recherche dans leur capacité d’objectiver leur « décision », en s’éloignant de l’expérience et des compétences des acteurs qui, eux, continuent de participer au système. Aborder les résultats de la recherche scientifique comme une production calibrée auprès des end-users avec tant de désinvolture ne peut que faciliter la croissance d’une doxa managériale qui se trouve des preuves à sa propre mesure et pour ses propres fins. Ce n’est pas tant que les « apprenants » présents à la formation (au formatage ?) soient ignorants de la situation, bien au contraire. Ils en sont des acteurs critiques pour certains.  - Monsieur l'enseignant-chercheur ! Ce qui ne va pas c'est le planter de drapeau, le planter de drapeau ne va pas du tout !- J'vais te le planter le drapeau moi , j'vais te le planter moi le drapeau ! (crédits : miwablo.com) Car, si on prend les choses avec un peu de recul, on découvrira que cette formation packagée constitue une belle opération de placement de produit – frais d’inscription mis à part, car c’est bel et bien un placement de produit payant. Les chercheurs du laboratoire viennent y présenter leurs différentes expertises auprès d’utilisateurs potentiels au sein d’universités partout à travers le monde. Ces derniers faciliteront une ultérieure contractualisation sur telle ou telle étude stratégique avec les instances décisionnaires de leur entité de rattachement. Ce cours devient ce faisant une courroie de transmission entre la recherche en scientométrie et l’usage d’indicateurs, et la méthode se fondre davantage et plus subtilement dans son objet. Côté pile, un observateur optimiste et bienveillant considérerait avec joie les mises en garde qui émaillent les différentes interventions sur les limites à donner aux analyses. Côté face, le peu de compétence concrète transmise aux participants ne peut faire qu’accroître le fossé entre une bonne conscience technique et une fausse maîtrise des enjeux sous-jacents. Et cela contribue probablement à la multiplication de ce type d’études, sur un marché que l’on imagine de plus en plus concurrentiel et profitable. Mais dans quelle mesure cette formation participe-t-elle à une dynamique internationale de normalisation de l’usage des indicateurs, cela reste à explorer… Show must go on, donc. Émilien Schultz (http://eschultz.fr/) Avec la relecture attentive et complétive de Jérôme Lamy et d'Arnaud Saint-Martin Image en bandeau : moulins près de Leiden, via espace-randonnee.fr.    [1] Un synopsis pour connecter les synapses : « The Centre for Science and Technology Studies (CWTS) studies the dynamics of scientific research and its connections to technology, innovation and society. This means studying scientific and academic research from a scientific point of view. » [2] Pascal Pansu, Nicoles Dubois, Jean-Léon Beauvois. Dis-moi qui te cite et je saurai ce que tu vaux, Grenoble, PUG, 2013. [3] Le lecteur souhaitant avoir une idée plus claire de la manière dont cette économie de la connaissance s’autonomise comme un enjeu à part entière pour les nouveaux « managers de la connaissance » trouvera quelques billes dans le livre de Dominique Foray, L’économie de la connaissance, Paris, La Découverte, 2010. [4] Yves Gingras, Les dérives de l’évaluation de la recherche. Du bon usage de la bibliométrie,  Paris, Raisons d’agir, 2014. [5] Je dis « on », mais on aura bien compris qu’il n’a rien du désignateur rigide : c’est tout le monde et n’importe qui, potentiellement vous et moi, enfin toutes celles et ceux qui, un jour ou l’autre, devront se mesurer à ces problèmes sur mesure qui ne peuvent que les concerner. [6] Pour une situation similaire dans le cas d’internet, voir Dominique Cardon, « Dans l’esprit du PageRank », Réseaux, n° 117,  2013, pp.  63-95. [7] Isabelle Bruno, À vos marques, prêts... cherchez ! La stratégie européenne de Lisbonne, vers un marché de la recherche, Bellecombe-en-Bauge, Éditions du Croquant, 2008. [8] Yves Gingras, Les dérives de l’évaluation de la recherche. Du bon usage de la bibliométrie, Paris, Raisons d’agir, 2014. [9] Claude Rosental, Les capitalistes de la science: enquête sur les démonstrateurs de la Silicon Valley et de la NASA, Paris, CNRS éditions, 2007. [10] Lorraine Daston, Peter Galison, Objectivité, trad., Paris, Les Presses du réel, 2012. [11] Voir aussi l’article de Nature paru en 2013. [12] Pour ceux qui ne voient pas le mode 2 partout dans les politiques de la recherche contemporaine, une petite précision s’impose : il s’agit d’une catégorisation proposé par Michael  Gibbons et al. dans The new production of knowledge: The dynamics of science and research in contemporary societies (Londres, Sage, 1994). Ce modèle déjà vieux de vingt ans découpait l’histoire de la recherche entre un régime disciplinaire classique qui appartiendrait largement au passé (le mode 1) et un régime contemporain marqué par l’interdisciplinarité, la mise en réseau et l’orientation vers la résolution de problèmes (sociaux, économiques, etc.) que les auteurs appelaient (de leurs vœux !) le mode 2.