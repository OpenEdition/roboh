AprÃ¨s vous avoir donnÃ© un panorama gÃ©nÃ©ral de lâ€™histoire de la lexicomÃ©trie, je voudrais maintenant vous montrer quels logiciels on peut utiliser, et surtout quelles Ã©tapes trÃ¨s concrÃ¨tes vous devrez suivre avant dâ€™analyser votre corpus [1]. Les commandements de la lexicomÃ©trie un. Avoir ses textes Ã  disposition Avant dâ€™Ãªtre analysÃ©s par un logiciel, vos textes doivent avoir Ã©tÃ© recopiÃ©s. Ce travail est particuliÃ¨rement chronophage. Il faut donc que vous vous assuriez que personne ne lâ€™ait dÃ©jÃ  entrepris avant vous. Si vous avez la chance que vos textes aient Ã©tÃ© numÃ©risÃ©s ou quâ€™ils aient Ã©tÃ© pris en photo, vous pourrez gagner du temps avec des logiciels de reconnaissance de caractÃ¨res. Cette application existe pour certains textes de Gallica (il sâ€™agit du mode texte), mais aussi sur google books. Les modernistes ainsi que les mÃ©diÃ©vistes auront sans doute moins de rÃ©sultats puisque ces logiciels sont perturbÃ©s par les lettrines et les orthographes exotiques des textes. Il ne faut donc pas sâ€™attendre Ã  des miracles. AprÃ¨s avoir acquis ces textes, il faudra les convertir obligatoirement en mode texte pour que le logiciel de lexicomÃ©trie puisse commencer Ã  fonctionner. Avoir recopiÃ© ses textes nâ€™est cependant pas suffisant, car le mode de constitution du corpus est dÃ©terminant. deux. Respecter des rÃ¨gles strictes de composition du corpus: Lâ€™analyse lexicomÃ©trique est de type statistique. Un traitement lexicomÃ©trique donnera toujours des rÃ©sultats, mais pour quâ€™ils aient un sens statistique [2] et quâ€™ils soient ainsi admis dans une rÃ©flexion historique, il faut redoubler dâ€™attention au moment de la constitution du corpus. Ces prÃ©cautions sont mises en Ã©vidence par <pers> Antoine Prost </pers> [3]. Il dÃ©gage trois rÃ¨gles essentielles, selon lesquelles le corpus doitÂ Ãªtre: HomogÃ¨neÂ : les textes doivent Ãªtre environ de mÃªme longueur, concerner le mÃªme public, porter sur le mÃªme thÃ¨me. Il mâ€™a ainsi Ã©tÃ© impossible dans mes propres recherches de comparer dans une mÃªme Ã©tude des sonnets et des Ã©pÃ®tres dÃ©dicatoires puisque ces deux textes ne sont pas de mÃªme nature. DiachroniqueÂ : les textes ne peuvent pas avoir Ã©tÃ© Ã©crits le mÃªme jour. ContrastÃ©Â : les textes doivent recÃ©ler des diffÃ©rences puisque ce sont justement ces diffÃ©rences que lâ€™on cherche Ã  observer. Les logiciels Les logiciels de lexicomÃ©trie sont foison, comme le soulignent Claire Lermercier et Claire Zalc [4]. Elles expliquent [5] nÃ©anmoins que les trois logiciels les plus sÃ©rieux pour faire une analyse lexicomÃ©trique sontÂ : <org> Alceste </org> [6], Lexico3 [7] et Hyperbase. Aucun dâ€™eux nâ€™est malheureusement libre de droit, et leur coÃt» varie de lâ€™un Ã  lâ€™autre, Alceste restant particuliÃ¨rement onÃ©reux. Lâ€™aspect pÃ©cuniaire nâ€™est Ã©videmment pas le seul critÃ¨re de distinction entre ces logiciels. La grandeur du corpus sâ€™avÃ¨re ainsi dÃ©terminant. <pers> Alceste </pers> accepte les corpus assez petits, ce qui nâ€™est pas le cas dâ€™Hyperbase. Les diffÃ©rences se font aussi sentir au niveau des options des logiciels. La <org> lemmatisation </org>, câ€™est-Ã -dire la simplification des diffÃ©rentes formes de mots [8], nâ€™est ainsi pas nÃ©cessaire avec <org> Alceste </org> ou Hyperbase alors quâ€™elle lâ€™est avec Lexico3. Hyperbase propose une analyse trÃ¨s fine des pronoms et des temps, ce que ne permet pas Lexico3, ou du moins pas directement, puisque cela exige une nouvelle lemmatisation. Il faut aussi prendre en compte la difficultÃ© dâ€™emploi du logiciel. Le maniement de Lexico3 est ainsi beaucoup plus aisÃ© que celui dâ€™Alceste [9]. Tous ces critÃ¨res sont donc dÃ©terminants pour choisir le logiciel. Leurs diffÃ©rences ne doivent cependant pas effrayer. Jâ€™ai ainsi soumis mon corpus Ã  deux logiciels diffÃ©rents (Ã  savoir <org> Alceste </org> et <org> Lexico3 </org>) pour voir si leurs rÃ©sultats diffÃ©raient. Ce ne fut pas le cas, bien au contraire. Par ailleurs, des lemmatisations diffÃ©rentes ou le recours Ã  des extensions du logiciel peuvent pallier Ã  certains manques du logiciel ÂÂ de« baseÂ Â» utilisÃ©. Ainsi CoocsÂ  dÃ©veloppÃ© par <pers> William Martinez </pers> [10], permet dâ€™Ã©tudier les cooccurrences, ce que ne permettait pas Lexico3 initialement. AprÃ¨s avoir choisi le logiciel, il faut passer par plusieurs Ã©tapes que je vais maintenant dÃ©tailler. Le problÃ¨me spÃ©cifique du grecÂ ancien: Utiliser un logiciel de <loc> lexicomÃ©trie </loc> sur un corpus en grec sâ€™avÃ¨re assez compliquÃ©. Lâ€™alphabet grec mais surtout les esprits et les accents ont rendu trÃ¨s difficile le traitement lexicomÃ©trique de ces textes. Ce problÃ¨me est en train dâ€™Ãªtre rÃ©glÃ© par certaines Ã©quipes de recherches, qui tentent de le dÃ©passer [11], mais il reste trÃ¨s difficile dâ€™utiliser la <org> lexicomÃ©trie </org> sur ce type de texte. Quand vous Ãªtes sÃr» que votre corpus satisfait Ã  ces diffÃ©rents critÃ¨res, vous devez commencer Ã  prÃ©parer vos textes pour lâ€™analyse. Les Ã©tapes prÃ©paratoires Ã  lâ€™analyse lexicomÃ©trique Le balisage du corpus Lâ€™ordinateur ne peut pas faire la diffÃ©rence entre les textes dâ€™un corpus. Il faut donc les lui indiquer par des balises. Celles-ci se prÃ©sentent de la faÃ§on suivante, pour le logiciel lexico3Â : &#60auteur=ronsard&#62;; Elles suivent plusieurs rÃ¨glesÂ : pas dâ€™espace entre les mots, pas de majuscules, pas dâ€™accent. Au-delÃ  dâ€™un simple dÃ©coupage (appelÃ© aussi segmentation) du corpus, cela permet Ã  lâ€™historien de sÃ©lectionner des points dâ€™entrÃ©es sur son corpus, des questionnements. Jâ€™avais ainsi choisi de me demander si les diffÃ©rences que je notais au sein du corpus Ã©taient dues Ã  la personnalitÃ© des auteurs ou aux dates dâ€™Ã©criture. Cela mâ€™a permis de sÃ©lectionner bien dâ€™autres balises que la simple balise auteur. Jâ€™ai choisi de baliser mon corpus des balises suivantesÂ : auteur, profession de lâ€™auteur, date de publication, republication Ã©ventuelle et titre de lâ€™Å“uvre. (Vous pouvez bien sÃr» en mettre autant que vous voulezÂ ). Le moment du balisage est donc bien un temps de rÃ©flexion qui permet de sâ€™interroger sur son propre sujet. Bien souvent on revient sur le balisage pour le prÃ©ciser ou au contraire supprimer certaines balises inutiles. Jâ€™ai ainsi supprimÃ© la balise ÂÂ mÃ©tatexteÂ Â»« qui mâ€™indiquait si lâ€™Ã©pÃ®tre dÃ©dicatoire Ã©tait suivie dâ€™un sonnet, car elle nâ€™apportait, en dÃ©finitive, rien de plus Ã  lâ€™analyse. ÂÂ Qui« lemmatise, dilemme attiseÂ Â[12]» La lemmatisation est la seconde Ã©tape de la plupart des analyses lexicomÃ©triques. Voici en quoi elle consiste. Les mots prennent diffÃ©rentes formesÂ dans la phrase. Ils peuvent Ãªtre mis au pluriel, Ãªtre conjuguÃ©sâ€¦ Ces diverses formes peuvent cependant Ãªtre ramenÃ©es Ã  une forme unique, celui du lemme. AinsiÂ : vois, voit, voyons, vue, vues, voyant, vis, verra, verrez, verront sont des formes diffÃ©rentes de voir. La <org> lemmatisation </org> consiste Ã  trouver les diffÃ©rentes formes revÃªtues par les mots du texte et Ã  les simplifier en une seule forme. Les verbes sont ramenÃ©s, la plupart du temps, Ã  lâ€™infinitif, les adjectifs Ã  leur masculin singulier et les noms Ã  leurs singuliers. Elle consiste aussi Ã  repÃ©rer les homographes afin dâ€™introduire des diffÃ©rences graphiques pour que lâ€™ordinateur puisse les diffÃ©rencier. ÂÂ EstÂ Â»« peut Ãªtre en effet une troisiÃ¨me personne du pluriel ou un point cardinal. Pour ne pas que le logiciel les confonde, il faudra coder le point cardinal en Est*. Dans certains cas, la <org> lemmatisation </org> devient une opÃ©ration vraiment trÃ¨s difficile. Je travaille sur la Reine Marguerite dâ€™un point de vue politique. Les titres quâ€™elles portait Ã©taient donc essentiels pour moi. Lorsque jâ€™ai voulu les lemmatisÃ©s, je me suis rendu compte que le mot princesse pouvait avoir les orthographes suivantesÂ : Princesse, PRINCESSE, princesse. Ces trois syntagmes ont le mÃªme sens politique, mais la question Ã©tait de savoir si je devais simplifier les majuscules en minuscules ou si je perdais un sens politique important. Jâ€™ai finalement dÃ©couvert que les Ã©diteurs mettaient les majuscules un peu au hasard, en fonction du matÃ©riel dont ils disposaient, ce qui mâ€™a permis de trancher. Cet exemple montre bien Ã  quel point lemmatiser nâ€™est pas chose facile et peut devenir compliquÃ©. La lemmatisation prend une part plus ou moins importante dans le travail du chercheur. Les contemporanÃ©istes ont souvent la possibilitÃ© de laisser les lemmatiseurs automatiques faire cet exercice. Les modernistes et les mÃ©diÃ©vistes ont un peu moins de chance, car il est prÃ©fÃ©rable quâ€™ils procÃ¨dent eux-mÃªmes Ã  la lemmatisation. Ils travaillent en effet sur des textes qui ne suivent pas les rÃ¨gles dâ€™orthographe qui leur sont postÃ©rieures, et lâ€™usage de certains mots a Ã©tÃ© perdu et reste inconnu des logiciels conÃ§us pour des textes contemporains. Â  Cette opÃ©ration a Ã©tÃ© et reste une cause de disputes assez violentes entre ceux qui acceptent ce principe et ceux qui le rejettent au nom de la linguistique. Lemmatiser implique en effet de perdre une grande partie de lâ€™information contenue dans un texte. En lemmatisant, je suis passÃ©e de douze mille Ã  six mille formes. Les tenants de la <org> non-lemmatisation </org> considÃ¨rent que cette perte est inacceptable puisque le chercheur intervient pour choisir quel mot est digne ou pas de figurer dans leur liste alors que des variations trÃ¨s fines des formes peuvent Ãªtre trÃ¨s rÃ©vÃ©latrices. Perdre la diversitÃ© des items reviendrait Ã  renoncer Ã  lâ€™objectif mÃªme de ceux qui voulaient justement Ã©tudier le langage. Câ€™est pour ces raisons que les premiers chercheurs[13] en lexicomÃ©trie nâ€™ont pas lemmatisÃ© leurs textes. Pour ma part, jâ€™ai choisi de cÃ©der aux sirÃ¨nes de la <org> lemmatisation </org> et ce pour plusieurs raisons. Je pense que lâ€™argument principal des linguistes, qui consiste Ã  dire que lâ€™on perd le sens du texte en lemmatisant ne fonctionne pas en histoire. <pers> Antoine Prost </pers> insiste ainsi sur cette diffÃ©rence ontologique qui diffÃ©rencie lâ€™historien du linguiste. ÂÂ Lâ€™historien« (â€¦) examine le vocabulaire pour autre chose que lui-mÃªme. (â€¦) Lâ€™historien ne cherche pas la mÃªme chose que le linguiste[14]Â Â». Son objectif Ânâ€™est« pas la connaissance du vocabulaire en lui mÃªme et pour lui-mÃªme, du systÃ¨me structurÃ© quâ€™il constitue, câ€™est-Ã -dire au sens strict, du lexique, mais celle des mentalitÃ©s et des attitudes latentes qui sâ€™y manifestent[15]Â Â». Cette diffÃ©rence dâ€™intÃ©rÃªt portÃ© au texte permet de le simplifier de faÃ§on Ã  mieux lâ€™analyser. La question de fond est celle-ciÂ : importe t-il rÃ©ellement Ã  un historien de savoir si le locuteur a utilisÃ© le participe ou la troisiÃ¨me personne du singulier dâ€™un verbe, ou de savoir quel verbe ce dernier a utilisÃ©Â ? Je crois que cette question est rhÃ©torique. Ce qui nous importe nâ€™est effectivement pas de savoir quelle forme de mot a Ã©tÃ© prÃ©fÃ©rÃ©e Ã  une autre, mais quel lemme a Ã©tÃ© choisi. Si effectivement, dans lâ€™analyse dâ€™un discours politique par exemple, on souhaite tout de mÃªme avoir accÃ¨s Ã  ce type dâ€™information, il reste tout Ã  fait possible de la faire en mettant <amount> en place une seconde </amount> lemmatisation qui codera le type dâ€™information souhaitÃ©, puisque lâ€™on peut toujours revenir au corpus non lemmatisÃ©. Cette simplification comporte des bÃ©nÃ©fices, souvent laissÃ©s de cÃ´tÃ© alors quâ€™ils sont centraux. Lemmatiser permet de mettre en Ã©vidence les hapax, ces mots qui nâ€™apparaissent quâ€™une seule fois dans un texte. Bien que leur analyse soit centrale, le refus de la lemmatisation les noie au milieu des formes rares des verbes. Par ailleurs, comme lâ€™a soulignÃ© <pers> Damon Mayaffre[16] </pers>, le fait de ne pas lemmatiser empÃªche dâ€™introduire graphiquement des diffÃ©rences entre les homographes. Lâ€™ordinateur compte alors juge sans se prÃ©occuper de savoir sâ€™il sâ€™agit du verbe Ã  la troisiÃ¨me personne du singulier ou bien du nom. Il admet[17] que la non prise en compte des doublons a sans doute faussÃ© en partie les rÃ©sultats de sa thÃ¨se. Lemmatiser, loin dâ€™Ãªtre ÂÂ un« luxe inutileÂ Â» serait donc bien un moment essentiel dans lâ€™analyse lexicomÃ©trique. Plus important encore, la <org> lemmatisation </org> permet Ã  lâ€™historien de beaucoup mieux connaÃ®tre son corpus, car il est obligÃ© dâ€™analyser chaque mot contenu dans ce dernier. Avant de dÃ©cider sâ€™il lemmatise ou non un mot, il utilise le logiciel pour le mettre en contexte et voir comment se dernier fonctionne. Cette opÃ©ration trÃ¨s longue[18] est donc un immense apport pour lâ€™historien car elle lui donne la possibilitÃ© de lire son texte dâ€™une autre faÃ§on et de le connaÃ®tre Ã  une autre Ã©chelle, celui du mot. Lâ€™exercice intellectuel exigÃ© par la <org> lemmatisation </org> est donc un moyen de progresser sur sa connaissance dâ€™un corpus. Cette lecture structurÃ©e est nÃ©cessaire. <pers> Antoine Prost </pers> enjoint dâ€™ailleurs ses collÃ¨gues Ã  la pratiquer pour dÃ©gager de nouvelles pistes de travail, car ÂÂ Lâ€™analyse« structurale, au contraire, sâ€™efforce de reconstituer des constellations de vocables, en repÃ©rant entre eux soit des liens de solidaritÃ©, dâ€™appel rÃ©ciproque, soit de substitution, soit dâ€™exclusionÂ : on dÃ©couvre ainsi une structure qui rÃ©vÃ¨le le temps rÃ©elÂ Â»[19]. Enfin, lâ€™argument selon lequel on lemmatiserait de faÃ§on automatique, sans se prÃ©occuper du sens des mots est un leurre puisque cette opÃ©ration implique une contextualisation automatique de chacun des mots que lâ€™on choisit de lemmatiser. MalgrÃ© les problÃ¨mes que pose lâ€™Ã©tape de la lemmatisation, le rapport bÃ©nÃ©fice/risque de cette technique semble pencher clairement en sa faveur. Cette Ã©tape passÃ©e, les premiers rÃ©sultats arrivent enfin. DiffÃ©rents types de rÃ©sultats Les rÃ©sultats dâ€™une analyse lexicomÃ©trique sont de nature trÃ¨s diffÃ©rente. Ils peuvent Ãªtre trÃ¨s <pers> simplesÂ  AFC </pers> (Analyse Factorielle des Correspondances[20]) Outil le plus puissant de lâ€™analyse lexicomÃ©trique. Il permet de visualiser les rÃ©partitions des mots au sein dâ€™un corpus. Les mots qui se retrouvent au centre du graphique sont utilisÃ©s par tous les textes. Les mots qui forment une corolle autour des titres sont communs Ã  certains textes en particulier. Les Analyses factorielles permettent donc de faire des regroupements au sein du corpus, de comprendre pourquoi certains textes sâ€™opposent Ã  dâ€™autres sur le plan du lexique. Elles permettent aussi de voir lâ€™Ã©volution du vocabulaire en fonction du temps, si lâ€™on a pris soin de faire une balise date. Cette premiÃ¨re analyse dÃ©gage un certain nombre de mots objectivement intÃ©ressants et avec lesquels on peut lancer dâ€™autres types de tests. Voici les principaux outils de lâ€™analyse lexicomÃ©trique. Lorsque lâ€™on analyse une AFC, on essaie de comprendre la formation de ces groupes et de ses oppositions, et on tente de les justifier. On cherche aussi au sein de ces AFC Ã  reconnaÃ®tre des figures idÃ©ales de rÃ©partition des mots. Lâ€™une de ses figures est le torck. Vous constatez Ã  droite quâ€™il y a plusieurs groupes de mots agglutinÃ©s et que les mots qui sont communs Ã  chacun de ces textes permettent le passage du vocabulaire spÃ©cifique dâ€™un texte au vocabulaire dâ€™un autre texte. Lors de lâ€™analyse on va dire quâ€™il y a un tork en faisant comme si ce passage se faisait de faÃ§on continue. Voici un second schÃ©ma dâ€™analyse comprenant, de faÃ§on explicite le tork: Dans un mÃ©moire, on met en valeur le rÃ©sultat ÂÂ graphiqueÂ Â»« de lâ€™AFC, qui ne peut reprÃ©senter que deux facteurs Ã  la fois, mais au cours de la recherche, il faut savoir que lâ€™on analyse des tableaux (voir image ci-jointe) et que lâ€™on observe la rÃ©partition et le sens des mots au sein des dix premiers facteurs. On sÃ©lectionne ensuite les facteurs qui nous apportent le plus dâ€™information ou qui sont le plus intÃ©ressants dans le cadre de notre problÃ©matique. Si les AFC sont le premier point dâ€™entrÃ©e de lâ€™analyse lexicomÃ©trique, elles peuvent et doivent Ãªtre complÃ©tÃ©es par dâ€™autres outils. Voici les principaux outils de l' analyse lexicomÃ©trique: La <org> Contextualisation </org> Elle permet de vÃ©rifier le sens dâ€™un mot. TrÃ¨s souvent, on a en effet une impression de proximitÃ© avec le texte, qui nous fait perdre de vue que notre emploi de la langue franÃ§aise est trÃ¨s diffÃ©rent de celui des personnes que lâ€™on Ã©tudie. Jâ€™ai pu ainsi constater que le substantif ÂÂ pouvoirÂ Â»« nâ€™est pratiquement jamais utilisÃ© dans les textes que jâ€™Ã©tudie. Voici un exemple de contextualisation possible, obtenu Ã  partir de mon corpus en utilisant <org> Alceste </org>: La ReprÃ©sentation dâ€™une frÃ©quence dâ€™apparition dâ€™un mot GrÃ¢ce Ã  cette option, on peut voir reprÃ©senter lâ€™usage dâ€™un mot et son Ã©volution au cours du temps. Je lâ€™ai utilisÃ© pour faire des ÂÂ zoomsÂ Â»« trÃ¨s prÃ©cis sur certains items et cela a Ã©tÃ© trÃ¨s efficace. Cette option pose cependant un problÃ¨me mÃ©thodologique si lâ€™on nâ€™a pas utilisÃ© une analyse factorielle des correspondances pour identifier le mot que lâ€™on veut Ã©tudier. Voici lâ€™un des diagrammes que jâ€™ai pu obtenir avec lexico3. Il sâ€™agissait pour moi de comprendre comment variait lâ€™emploi des titres ÂÂ PrincesseÂ Â»,« ÂÂ Duchesse« de ValoisÂ Â» et ÂÂ ReineÂ Â»« au cours du temps. Jâ€™ai tout simplement demandÃ© au logiciel de me sortir ce diagramme, que jâ€™ai pu interprÃ©ter sur un plan politique en le mettant en parallÃ¨le avec les dates clÃ©s de la vie de Marguerite de Valois: Les cooccurrences Outil trÃ¨s puissant servant Ã  comprendre comment fonctionne un mot avec les autres mots. Lorsque lâ€™on utilise Lexico3, on ne peut pas avoir immÃ©diatement accÃ¨s aux outils de cooccurrences. Le logiciel Cooccs, dÃ©veloppÃ© par <pers> William Martinez </pers> est un excellent moyen de pallier Ã  ce problÃ¨me. Il propose de nombreuses applications. On peut ainsi voir quels sont les mots qui apparaissent le plus souvent avec une forme pÃ´le. On peut mÃªme en avoir une reprÃ©sentation schÃ©matique. Voici ci-dessous une reprÃ©sentation possible des relations entre les mots de mon corpus. Cette reprÃ©sentation est, en elle mÃªme impossible Ã  analyser. Mais, en choisissant une probabilitÃ© dâ€™apparition moins forte il est facile dâ€™obtenir des ÂÂ zoomsÂ Â»« (voir ici) sur des relations spÃ©cifiques entre les mots et devoir comment les mots sâ€™enchaÃ®nent les uns avec les autres. Ici encore, la sÃ©lection des mots que lâ€™on Ã©tudie nâ€™a rien dâ€™arbitraire: on regarde les cooccurrences de tous les mots qui se sont rÃ©vÃ©lÃ©s contributifs lors des premiÃ¨res AFC Les Segments rÃ©pÃ©tÃ©s Outil permettant de mettre en Ã©vidence les mots qui fonctionnent ensembles dâ€™une faÃ§on rÃ©pÃ©tÃ© et ainsi de voir leur variation. Jâ€™ai pu ainsi observer Ã  la loupe les signature de mes Ã©pÃ®tres dÃ©dicatoires et avoir une reprÃ©sentation graphique de leur rÃ©partition en fonction du temps. Conclusion La <org> lexicomÃ©trie </org> constitue un formidable outil heuristique pour les historiens. Elle permet de comprendre le fonctionnement interne dâ€™un corpus grÃ¢ce aux AFC, de voir quels sont les mots les plus employÃ©s et dâ€™en apercevoir lâ€™Ã©volution en fonction du temps. Ces outils ne sont malheureusement accessibles quâ€™au prix dâ€™un grand travail prÃ©paratoires de lemmatisation et de mise en place des balises. Ce regard complÃ©mentaire aux mÃ©thodes plus classiques de la base de donnÃ©es ou dâ€™une lecture trÃ¨s documentÃ©e des textes est extrÃªmement performante. [1] Pour ce faire, je me fonderai encore une fois sur mes propres lectures et sur les confÃ©rences et les cours que jâ€™ai reÃ§us Ã  <loc> Paris </loc> <loc>I </loc> et Ã  lâ€™EN.S. <pers> Ulm </pers>. [2] Je reprends ici une partie du cour de <pers> Benjamin </pers> Deruelle consultable sur le siteÂ : http//tdhist.univ-paris1.fr/.: [3] <pers> Antoine Prost </pers>, Vocabulaire des proclamations Ã©lectorales de mille huit cent quatre-vingt-un, 1885 et <time> mille huit cent quatre-vingt-neuf </time>, <loc>Paris </loc>, <time>PUF mille neuf cent soixante-treize </time>, 192 p. [4] <pers> Lemercier </pers> Claire et Zalc Claire, MÃ©thodes quantitatives pour l' historien, <loc> Paris </loc>, <time>La DÃ©couverte deux mille huit </time>, 120 pÂ . [5] Sur leur site ÂÂ quanti« RHMCÂ Â» adresseÂ : http//www.quanti.ihmc.ens.fr/Lexicometrie-les-logiciels.html.: Lien valide le 30/01/12. [6] Logiciel conÃ§u par <pers> Max Reinert </pers> dans les annÃ©es quatre-vingt-dix. Pour en savoir plus voirÂ : http//www.image-zafar.com/index_alceste.htm.: Hyperbase a Ã©tÃ© crÃ©Ã© dans les annÃ©es quatre-vingts pour mettre les textes de la RÃ©volution Ã  disposition. Voir http//www.unice.fr/bcl/spip.php?rubrique38..: [7] Logiciel inventÃ© et dÃ©veloppÃ© par <pers> AndrÃ© Salem </pers> et <pers> Ludovic Le Bart </pers> dans les annÃ©es  <time> mille neuf cent quatre-vingt-dix </time>. [8] Je vous renvoie Ã  la suite de ce billet qui explique en quoi la lemmatisation consiste. [9] Je vous conseille dâ€™aller voir Ã  lâ€™adresse suivante, signalÃ©e par <pers> Philippe Cibois </pers> sur le blogÂ Â : http//textometrie.ens-lyon.fr/spip.php?article96: (lien valide le 30/01/12). Vous y trouverez les diffÃ©rents manuels dâ€™utilisation des logiciels. Vous pourrez ainsi choisir au mieux celui qui sera plus rentable pour voir. Consultez aussi le site du ÂÂ quanti.RHMCÂ Â».,« il est trÃ¨s bien construit. [10] Voir le site de <pers> William </pers> MartinezÂ : http//williammartinez.fr/coocs/page.php.: (lien valide le 30/01/12/) [11] http//lexicometrica.univ-paris3.fr/thema/thema1/spec1-texte3.pdf.: lien valide le 30/01/12. [12] http//lexicometrica.univ-paris3.fr/article/numero2/brunet2000.html.: [13] <pers> ENS St Cloud </pers>. [14] <pers> Antoine Prost </pers>, Vocabulaire des proclamations Ã©lectorales, Idem, p6. [15] Ibidem, p5-6.. [16] <pers> Damon </pers> Mayaffre ÂÂ De« la lexicomÃ©trie Ã  la logomÃ©trieÂ», consultable Ã  lâ€™adresseÂ : (lien valide le <org> 30/01/12 http//www.arts.uottawa.ca/astrolabe/articles/art0048/Logometrie.htm#Lemmatiser%20=%20d%E9grouper </org>: [17] <pers> Damon </pers> Mayaffre, Ibidem. [18] Il mâ€™a fallu <amount> plus dâ€™un mois </amount> pour lemmatiser mon corpus. [19] <pers> Antoine Prost </pers>, Vocabulaire des proclamations Ã©lectorales, OpCit,. p12.. [20] Les concepts mathÃ©matiques sur lesquels reposent les AF.C. sont assez complexes. Pour en savoir plus, je vous conseille de lire <pers> Philippe </pers> CiboisÂ , Lâ€™Analyse factorielle, analyse en composantes principales et analyse des correspondances, <org> Paris </org>, PUF, le point des connaissances1983,, cent vingt-six p., mais aussi le cour dâ€™Alain Guerreau. Statistiques pour historiens, cours pour lâ€™Ã‰cole des ChartesÂ : http//elec.enc.sorbonne.fr/statistiques/stat2004.pdf.: Lien valide le <org> 30/01/12 </org>.
